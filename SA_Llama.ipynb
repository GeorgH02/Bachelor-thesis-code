{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations for colab\n",
    "#!pip install huggingface_hub transformers torch accelerate datasets peft urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface login necessary because llama is gated\n",
    "# token should be invalidated and refreshed after every git commit\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets_bigtech_10k_sample = pd.read_csv(\"./tweets_bigtech_10ksample.csv\")\n",
    "df_tweets_bigtech_10k_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model using peft to be able to fine-tune\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "     task_type=TaskType.SEQ_CLS,\n",
    "     inference_mode=False,\n",
    "     r=8,\n",
    "     lora_alpha=16,\n",
    "     lora_dropout=0.1\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with pure llama-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = \n",
    "pipeline(\"The new Iphone is not good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k_sample[\"predicted_sentiment\"] = df_tweets_bigtech_10k_sample[\"text\"].apply(lambda x: pipeline(x)[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k_sample[\"predicted_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapting predicted labels to fit format\n",
    "df_tweets_bigtech_10k_sample[\"predicted_sentiment\"] = df_tweets_bigtech_10k_sample[\"predicted_sentiment\"].apply(lambda x: 2 if x == \"LABEL_1\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing neutral tweets, since model only predicted negative and positive\n",
    "df_tweets_bigtech_10k_sample = df_tweets_bigtech_10k_sample[df_tweets_bigtech_10k_sample[\"sentiment\"] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k_sample[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k_sample[\"predicted_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def performance_metrics(df, label, prediction):\n",
    "     accuracy = accuracy_score(df[label], df[prediction])\n",
    "     precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "     recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "     f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "     print(f\"Accuracy: {accuracy}\")\n",
    "     print(f\"Precision: {precision}\")\n",
    "     print(f\"Recall: {recall}\")\n",
    "     print(f\"F1-Score: {f1}\")\n",
    "\n",
    "performance_metrics(df_tweets_bigtech_10k_sample, \"sentiment\", \"predicted_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting polarity to sentiment and trying to get llama to predict three labels (negative, neutral, positive)\n",
    "def polarity_to_sentiment(polarity):\n",
    "    if -0.2 <= polarity <= 0.2:\n",
    "        return 1\n",
    "    if polarity > 0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_tweets_bigtech_10k_sample[\"sentiment\"] = df_tweets_bigtech_10k_sample[\"polarity\"].apply(polarity_to_sentiment)\n",
    "df_tweets_bigtech_10k_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_tweets_bigtech_10k_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning with Tweets BigTech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3 # specifying num_labels resolves runtime CUDA error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token definitions for both tokenizer and model allow for batch size > 1\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"/content/tweets_bigtech_20ksample.csv\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# model expects column explicitly called \"labels\"\n",
    "df = df.rename({\"sentiment\" : \"labels\"}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of the low-rank adaptation\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\"],#, \"v_proj\"],  # target modules for LoRA\n",
    "    lora_dropout=0.1,  # dropout rate\n",
    "    bias=\"none\",  # bias handling\n",
    "    task_type=\"SEQ_CLS\" # sentiment analysis is a form of sequence classification\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# experimenting with different training argument definitions\n",
    "\n",
    "# batch size >= 64 results in out of memory error even with best GPU (A100)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-sentiment-analysis\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs) # removing num_items_in_batch\n",
    "        # saving past state if it exists\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # not using .loss here since the model may return tuples instead of ModelOutput\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./llama2-sentiment-analysis-finetuned\")\n",
    "tokenizer.save_pretrained(\"./llama2-sentiment-analysis-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model and applying it\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "llama_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./llama2-sentiment-analysis-finetuned\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3 # specifying num_labels resolves runtime CUDA error\n",
    ")\n",
    "\n",
    "tokenizer_llama_ft = AutoTokenizer.from_pretrained(\"./llama2-sentiment-analysis-finetuned\")\n",
    "\n",
    "# moving model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llama_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_test = pd.DataFrame(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_llama_ft(df_tweets_bigtech_test[\"text\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokens = {key: value.to(device) for key, value in tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_ft.config.pad_token_id = llama_ft.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# setting the model to evaluation-mode\n",
    "llama_ft.eval()\n",
    "\n",
    "batch_size = 32\n",
    "num_batches = len(df_tweets_bigtech_test) // batch_size + 1\n",
    "predictions = []\n",
    "\n",
    "# performing evaluation in batches to not overload GPU\n",
    "for i in range(num_batches):\n",
    "  start_idx = i * batch_size\n",
    "  end_idx = min((i + 1) * batch_size, len(df_tweets_bigtech_test))\n",
    "  batch_tokens = tokenizer_llama_ft(df_tweets_bigtech_test[\"text\"][start_idx:end_idx].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  batch_tokens = {key: value.to(device) for key, value in batch_tokens.items()}\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = llama_ft(**batch_tokens)\n",
    "      batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "      predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "df_tweets_bigtech_test[\"llama_ft_prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_pred(model, tokenizer, df, text_col):\n",
    "    # setting padding tokens\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(model.device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_tokens = tokenizer(df[text_col][start_idx:end_idx].tolist(), padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        batch_tokens = {key: value.to(device) for key, value in batch_tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "    df[\"llama_ft_prediction\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def performance_metrics(df, label, prediction):\n",
    "     accuracy = accuracy_score(df[label], df[prediction])\n",
    "     precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "     recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "     f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "     print(f\"Accuracy: {accuracy}\")\n",
    "     print(f\"Precision: {precision}\")\n",
    "     print(f\"Recall: {recall}\")\n",
    "     print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_test, \"labels\", \"llama_ft_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning with brand sa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_brd_tr = pd.read_csv(\"Dataset - Train.csv\")\n",
    "df_brd_tst = pd.read_csv(\"Dataset - Test.csv\")\n",
    "df_brd = pd.concat([df_brd_tr, df_brd_tst])\n",
    "df_brd = df_brd.drop(\"Tweet\", axis=1)\n",
    "df_brd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd = df_brd[df_brd[\"tweet_text\"].notna()]\n",
    "len(df_brd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd[\"labels\"] = df_brd[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace({\"Negative emotion\" : 0, \"Positive emotion\" : 2, \"No emotion toward brand or product\" : 1, \"I can't tell\" : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()\n",
    "\n",
    "def format_tweet(tweet):\n",
    "    if not isinstance(tweet, str):\n",
    "        return tweet\n",
    "    # mask web urls\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, \"{{URL}}\")\n",
    "    # format twitter account\n",
    "    tweet = re.sub(r\"\\b(\\s*)(@[\\S]+)\\b\", r'\\1{\\2@}', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd[\"text\"] = df_brd[\"tweet_text\"].apply(format_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_brd)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-sentiment-analysis\",\n",
    "    per_device_train_batch_size=40,\n",
    "    per_device_eval_batch_size=40,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./llama2-sentiment-analysis-finetuned-brdsa\")\n",
    "tokenizer.save_pretrained(\"./llama2-sentiment-analysis-finetuned-brdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "model_ft = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"llama-2-7b-sentiment-analysis-finetuned-brdsa\"\n",
    ")\n",
    "\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"llama-2-7b-sentiment-analysis-finetuned-brdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10k = pd.read_csv(\"tweets_bigtech_10ksample.csv\")\n",
    "df_tweets_bigtech_10k.dropna(inplace=True)\n",
    "df_tweets_bigtech_2k = df_tweets_bigtech_10k.sample(n=2000)\n",
    "df_tweets_bigtech_2k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pred(model_ft, tokenizer_ft, df_tweets_bigtech_2k, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_2k, \"sentiment\", \"llama_ft_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# second evaluation function to confirm accuracy of results\n",
    "def evaluate_sentiment(df, true_col, pred_col):\n",
    "    eval_df = df.dropna(subset=[true_col, pred_col])\n",
    "\n",
    "    accuracy = accuracy_score(eval_df[true_col], eval_df[pred_col])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        eval_df[true_col],\n",
    "        eval_df[pred_col],\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    print(classification_report(eval_df[true_col], eval_df[pred_col]))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"num_samples\": len(eval_df)\n",
    "    }\n",
    "\n",
    "metrics = evaluate_sentiment(df_tweets_bigtech_2k, \"sentiment\", \"llama_ft_prediction\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pred(model_ft, tokenizer_ft, df_brd, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_brd, \"labels\", \"llama_ft_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning with sentiment corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama_t = AutoTokenizer.from_pretrained(model_name)\n",
    "llama = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3 # specifying num_labels resolves runtime CUDA error\n",
    ")\n",
    "\n",
    "llama_t.pad_token = llama_t.eos_token\n",
    "llama.config.pad_token_id = llama.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "llama = prepare_model_for_kbit_training(llama)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank of the low-rank adaptation\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],  # target modules for LoRA\n",
    "    lora_dropout=0.05,  # dropout rate\n",
    "    bias=\"none\",  # bias handling\n",
    "    task_type=\"SEQ_CLS\" # sentiment analysis is a form of sequence classification\n",
    ")\n",
    "\n",
    "llama = get_peft_model(llama, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(df_sc)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return llama_t(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# batch size >= 64 results in out of memory error even with best GPU (A100)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-sentiment-analysis\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    save_total_limit=2,\n",
    "    label_names=label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=llama,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=llama_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.save_pretrained(\"./llama2-sentiment-corpus\")\n",
    "llama_t.save_pretrained(\"./llama2-sentiment-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment-llama-2-7b\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=7,\n",
    "    save_total_limit=7,\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llama,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=llama_t\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_llama = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "val_df = val_dataset.to_pandas()\n",
    "\n",
    "id_to_sentiment = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "folders = [113, 226, 339, 452, 565, 678, 784]\n",
    "for epoch in range(7):\n",
    "    foldernr = folders[epoch]\n",
    "    checkpoint_dir = f\"./sentiment-llama-2-7b/checkpoint-{str(foldernr)}\"\n",
    "    print(f\"Loading model from {checkpoint_dir}\")\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    checkpoint_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "    val_df_copy = val_df.copy()\n",
    "    val_df_with_preds = llama_pred(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        val_df_copy,\n",
    "        text_col=\"text\"\n",
    "    )\n",
    "\n",
    "    true_labels = val_df_with_preds[\"labels\"].tolist()\n",
    "    pred_labels = val_df_with_preds[\"llama_ft_prediction\"].tolist()\n",
    "\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        target_names=list(id_to_sentiment.values()),\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    precision = report[\"weighted avg\"][\"precision\"]\n",
    "    recall = report[\"weighted avg\"][\"recall\"]\n",
    "    f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for sentiment_class in id_to_sentiment.values():\n",
    "        print(f\"{sentiment_class}: F1={report[sentiment_class]['f1-score']:.4f}, \" +\n",
    "              f\"Precision={report[sentiment_class]['precision']:.4f}, \" +\n",
    "              f\"Recall={report[sentiment_class]['recall']:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(f\"Best epoch according to F1-score: {best_epoch+1}, F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "model_corpus_ft = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./llama2-sentiment-corpus\"\n",
    ")\n",
    "\n",
    "tokenizer_corpus_ft = AutoTokenizer.from_pretrained(\"./llama2-sentiment-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_test = test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pred(model_corpus_ft, tokenizer_corpus_ft, df_corpus_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended function for performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def performance_metrics(df, label, prediction):\n",
    "    accuracy = accuracy_score(df[label], df[prediction])\n",
    "    precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "    recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "    f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "    report = classification_report(df[label], df[prediction], target_names=label_names, digits=4)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_corpus_test, \"labels\", \"llama_ft_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after new method with checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_llama = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "llama_corpus_4 = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    \"./sentiment-llama-2-7b/checkpoint-452\"\n",
    ")\n",
    "\n",
    "llama_corpus_4_t = AutoTokenizer.from_pretrained(\"./sentiment-llama-2-7b/checkpoint-452\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_pred(llama_corpus_4, llama_corpus_4_t, df_corpus_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_corpus_test, \"labels\", \"llama_ft_prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
