{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations for colab\n",
    "#!pip install transformers accelerate torch huggingface_hub datasets emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BerTweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bertweet-sentiment analysis\n",
    "# https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bertweet_t = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "bertweet = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sentiment_score(text, tokenizer, model):\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#def bert_pred(model, tokenizer, df, text_col):\n",
    "\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(model.device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_tokens = tokenizer(df[text_col][start_idx:end_idx].tolist(),\n",
    "                                 padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        batch_tokens = {key: value.to(device) for key, value in batch_tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "    df[\"bert_pred\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df_brd_sa = pd.read_csv(\"./analysis_data/Brand Sentiment Analysis Dataset/Dataset - Train.csv\")\n",
    "\n",
    "# read in data after NER csv\n",
    "df_brd_sa = pd.read_csv(\"./data after NER.csv\")\n",
    "\n",
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"sentiment_prediction\"] = df_brd_sa[\"tweet_text\"].apply(lambda x: sentiment_score(x[:500], bertweet_t, bertweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"sentiment_prediction\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 negative\n",
    "# 1 neutral\n",
    "# 2 positive\n",
    "df_brd_sa[\"sentiment\"] = df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace({\"Negative emotion\" : 0, \"Positive emotion\" : 2, \"No emotion toward brand or product\" : 1, \"I can't tell\" : 1})\n",
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def performance_metrics(df, label, prediction):\n",
    "     accuracy = accuracy_score(df[label], df[prediction])\n",
    "     precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "     recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "     f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "     print(f\"Accuracy: {accuracy}\")\n",
    "     print(f\"Precision: {precision}\")\n",
    "     print(f\"Recall: {recall}\")\n",
    "     print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_brd_sa, \"sentiment\", \"sentiment_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.to_csv(\"./data after SA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bertweet, bertweet_t, df_brd_sa, \"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_brd_sa, \"sentiment\", \"bertweet_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with sentiment corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bertweet, bertweet_t, df_sc, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_sc, \"labels\", \"bert_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bertweet fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to fine-tune this model (goal of accuracy > 72,89% and precision > 88,1%)\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_brd_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['tweet_text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column('sentiment', 'labels')\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, # adjust depending on resources\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/bertweetSA_after_inital_finetuning')\n",
    "tokenizer.save_pretrained('./models/bertweetSA_after_inital_finetuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing fine-tuned BERTweet-SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and trying the saved model\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./models/bertweetSA_after_inital_finetuning\")\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"./models/bertweetSA_after_inital_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"sentiment_prediction_finetuned\"] = df_brd_sa[\"tweet_text\"].apply(lambda x: sentiment_score(x[:500], tokenizer2, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very high results, but may not be too telling because it was tested on same data it was finetuned on\n",
    "performance_metrics(df_brd_sa, \"sentiment\", \"sentiment_prediction_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing Tweets Big Tech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tweets_bigtech_10ksample = pd.read_csv(\"tweets_bigtech_10ksample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating small test dataset with random sampling for later\n",
    "df_tweets_bigtech_test = df_tweets_bigtech.sample(n=4000, seed=42)\n",
    "df_tweets_bigtech_test.to_csv(\"./tweets_bigtech_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10ksample[\"sentiment_prediction_finetuned\"] = df_tweets_bigtech_10ksample[\"text\"].apply(lambda x: sentiment_score(x[:500], tokenizer2, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_10ksample, \"sentiment\", \"sentiment_prediction_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with not fine-tuned model on tweets-bigtech-data\n",
    "df_tweets_bigtech_10ksample[\"sentiment_prediction\"] = df_tweets_bigtech_10ksample[\"text\"].apply(lambda x: sentiment_score(x[:500], tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_10ksample, \"sentiment\", \"sentiment_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Fine-tuning of BERTweet-SA with big tech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning on tweets_bigtech_sample\n",
    "import pandas as pd\n",
    "df_tweets_bigtech_sample = pd.read_json(\"./tweets_bigtech_sample.json\", orient=\"records\")\n",
    "df_tweets_bigtech_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_tweets_bigtech_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return bertweet_t(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "#tokenized_datasets = tokenized_datasets.rename_column('sentiment', 'labels')\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_datasets.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, # adjust depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bertweet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=bertweet_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, # adjust depending on resources\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/bertweet-sa-after-tweetsbigtech\")\n",
    "bertweet_t.save_pretrained(\"./models/bertweet-sa-after-tweetsbigtech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_after_tweetsbigtech = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./models/bertweet-sa-after-tweetsbigtech\",\n",
    "    num_labels=3)\n",
    "tokenizer_after_tweetsbigtech = AutoTokenizer.from_pretrained(\"./models/bertweet-sa-after-tweetsbigtech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(model_after_tweetsbigtech, tokenizer_after_tweetsbigtech, df_sc_10ksample, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_sc_10ksample, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_test = pd.read_csv(\"./tweets_bigtech_test.csv\")\n",
    "df_tweets_bigtech_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sentiment_score(text, tokenizer, model):\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt', truncation = True, padding=\"max_length\") #max_length=128 necessary?\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_test[\"sentiment_prediction_finetuned\"] = df_tweets_bigtech_test[\"text\"].apply(lambda x: sentiment_score(x[:500], tokenizer_after_tweetsbigtech, model_after_tweetsbigtech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_test, \"sentiment\", \"sentiment_prediction_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning bertweet sa with corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bertweet_t = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "bertweet = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining length of tokens in the dataset to determine maxlength\n",
    "df_sc[\"text\"].str.split().str.len().agg([\"mean\",\"max\",\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sc)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return bertweet_t(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32, # adjust depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bertweet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=bertweet_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bertweet-sa-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "bertweet_corpus_t = AutoTokenizer.from_pretrained(\"./bertweet-sa-corpus\")\n",
    "\n",
    "bertweet_corpus = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./bertweet-sa-corpus\",\n",
    "    device_map=\"auto\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test_df = test_dataset.to_pandas()\n",
    "corpus_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def bert_pred(model, tokenizer, df, text_col):\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_texts = df[text_col][start_idx:end_idx].tolist()\n",
    "\n",
    "        batch_tokens = tokenizer(batch_texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        batch_tokens = {key: value.to(device) for key, value in batch_tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "    df[\"bert_pred_sa\"] = predictions\n",
    "\n",
    "bert_pred(bertweet_corpus, bertweet_corpus_t, corpus_test_df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def performance_metrics(df, label, prediction):\n",
    "    accuracy = accuracy_score(df[label], df[prediction])\n",
    "    precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "    recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "    f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "    report = classification_report(df[label], df[prediction], target_names=label_names, digits=4)\n",
    "    print(report)\n",
    "\n",
    "performance_metrics(corpus_test_df, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying model to case study data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dell = pd.read_json(\"./dell_cs_after_ner.json\", orient=\"records\")\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bertweet_corpus, bertweet_corpus_t, df_dell, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell.to_json(\"./dell_cs_after_predictions.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Roberta base sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "roberta_sentiment_t = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "roberta_sentiment = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model with portion of sentiment corpus to see performance on diverse data\n",
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc_10ksample = df_sc.sample(10000, random_state=42)\n",
    "df_sc_10ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying preprocessing from model's hf page to data before predicting def preprocess(text):\n",
    "# def preprocess_text(text):\n",
    "#     new_text = []\n",
    "#     for t in text.split(\" \"):\n",
    "#         t = '@user' if t == \"{{MENTION}}\" else t\n",
    "#         t = 'http' if t == \"{{URL}}\" else t\n",
    "#         new_text.append(t)\n",
    "#     return \" \".join(new_text)\n",
    "\n",
    "# df_sc_10ksample[\"text\"] = df_sc_10ksample[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# didnt improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(roberta_sentiment, roberta_sentiment_t, df_sc_10ksample, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_sc_10ksample, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_10ksample[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_10ksample[\"bert_pred_sa\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "s_classifier = pipeline(\"sentiment-analysis\", model= model_path, tokenizer=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_10ksample[\"prediction_pipeline\"] = df_sc_10ksample[\"text\"].apply(lambda x: s_classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc_10ksample[\"pipeline_label\"] = df_sc_10ksample[\"prediction_pipeline\"].apply(lambda x: x[0][\"label\"])\n",
    "df_sc_10ksample[\"pipeline_label\"] = df_sc_10ksample[\"pipeline_label\"].replace({\"negative\":0, \"neutral\":1, \"positive\":2})\n",
    "df_sc_10ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_sc_10ksample, \"labels\", \"pipeline_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning with corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sc)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return roberta_sentiment_t(example['text'], truncation=True,\n",
    "                       padding='max_length', max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./twitter-roberta-sentiment',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32, # adjust depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=roberta_sentiment,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=roberta_sentiment_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./twitter-roberta-sentiment-after-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "twitter_roberta_corpus_t = AutoTokenizer.from_pretrained(\"./twitter-roberta-sentiment-after-corpus\")\n",
    "\n",
    "twitter_roberta_corpus = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./twitter-roberta-sentiment-after-corpus\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_test = test_dataset.to_pandas()\n",
    "df_corpus_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(twitter_roberta_corpus, twitter_roberta_corpus_t, df_corpus_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_corpus_test, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning Bert-base-uncased with tweets bigtech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bert_base_t = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "bert_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 25k rows of Tweets-BigTech-data to finetune\n",
    "import pandas as pd\n",
    "df_tweets_bigtech_25ksample = pd.read_csv(\"./tweets_bigtech_25ksample.csv\")\n",
    "df_tweets_bigtech_25ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_tweets_bigtech_25ksample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return bert_base_t(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column('sentiment', 'labels')\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-base-uncased-sa',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, # adjust depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_base,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=bert_base_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base.save_pretrained(\"bert-base-uncased-sa\")\n",
    "bert_base_t.save_pretrained(\"bert-base-uncased-sa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bert_base_ft_t = AutoTokenizer.from_pretrained(\"bert-base-uncased-sa\")\n",
    "\n",
    "bert_base_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased-sa\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_test = test_dataset.to_pandas()\n",
    "df_tweets_bigtech_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model parameters to GPU if not loaded correctly\n",
    "# param_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bert_base_ft, bert_base_ft_t, df_tweets_bigtech_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_bigtech_test, \"labels\", \"bert_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also evaluating model on Brand-SA-data\n",
    "df_brd_sa_p = pd.read_csv(\"./brand_sentiment_analysis_preprocessed.csv\")\n",
    "df_brd_sa_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bert_base_ft, bert_base_ft_t, df_brd_sa_p, \"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_brd_sa_p, \"sentiment\", \"bert_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with Tweets Sentiment Classification data (only negative and positive)\n",
    "df_tweets_sc = pd.read_csv(\"./tweets_sentiment_classification_preprocessed.csv\")\n",
    "df_tweets_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bert_base_ft, bert_base_ft_t, df_tweets_sc, \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tweets_sc, \"label\", \"bert_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance for posts where model didnt predict neutral\n",
    "df_tweets_sc_np = df_tweets_sc[df_tweets_sc[\"bert_pred\"] != 1]\n",
    "performance_metrics(df_tweets_sc_np, \"label\", \"bert_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning bert base uncased with corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bert_base_t = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "bert_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sc = pd.read_json(\"sentiment_corpus.json\", orient=\"records\")\n",
    "df_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return bert_base_t(example['text'], truncation=True,\n",
    "                       padding='max_length', max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-base-uncased-sa',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32, # adjust depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_base,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=bert_base_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_base.save_pretrained(\"bert-base-sentiment-corpus\")\n",
    "# bert_base_t.save_pretrained(\"bert-base-sentiment-corpus\")\n",
    "trainer.save_model(\"bert-base-sentiment-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "bert_base_corpus_t = AutoTokenizer.from_pretrained(\"./bert-base-sentiment-corpus\")\n",
    "\n",
    "bert_base_corpus = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./bert-base-sentiment-corpus\",\n",
    "    num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_device = next(bert_base_corpus.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus_test = test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# version where tokenization happens at CPU and prediction on GPU,\n",
    "# bert tokenizer cant be moved to GPU\n",
    "def bert_pred(model, tokenizer, df, text_col):\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = len(df) // batch_size + 1\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_texts = df[text_col][start_idx:end_idx].tolist()\n",
    "\n",
    "        batch_tokens = tokenizer(batch_texts, padding=\"max_length\", truncation=True,\n",
    "                                 return_tensors=\"pt\", max_length=128)\n",
    "        batch_tokens = {key: value.to(device) for key, value in batch_tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "    df[\"bert_pred_sa\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bert_base_corpus, bert_base_corpus_t, df_corpus_test, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def performance_metrics(df, label, prediction):\n",
    "    accuracy = accuracy_score(df[label], df[prediction])\n",
    "    precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "    recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "    f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "    report = classification_report(df[label], df[prediction], target_names=label_names, digits=4)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_corpus_test, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbt_app = pd.read_json(\"./tweets_bigtech_10k_application_afterNER.json\", orient=\"records\")\n",
    "df_tbt_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred(bert_base_corpus, bert_base_corpus_t, df_tbt_app, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(df_tbt_app, \"labels\", \"bert_pred_sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbt_app.to_json(\"tweets_bigtech_10k_application_afterSA.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
