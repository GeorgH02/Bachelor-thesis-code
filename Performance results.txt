Performance results:

SA:

BerTWEET-Base-Sentiment-Analysis tested with 8600 rows of Brand Sentiment Analysis data (no own fine-tuning): 
Accuracy = 72,89%
Precision = 88,1%
Recall = 72,89%
F1 = 78,84%

BerTWEET-Base-Sentiment-Analysis tested with Tweets-BigTech data (no own fine-tuning):
Accuracy: 56,76%
Precision: 65,84%
Recall: 56,76%
F1-Score: 57,42%

BerTWEET-Base-Sentiment-Analysis tested with Sentiment corpus (no own fine-tuning):
Accuracy: 0.6272260790824027
Precision: 0.6611441262229791
Recall: 0.6272260790824027
F1-Score: 0.6272425239648367


BerTWEET-Base-Sentiment-Analysis fine-tuned with 8600 rows of Brand Sentiment Analysis data and 2 epochs, 
tested on Tweets Big Tech data: 
Accuracy = 56,34%
Precision = 58,9%
Recall = 56,34%
F1 = 48,42%

BerTWEET-Base-Sentiment-Analysis fine-tuned with 10.000 rows of Tweets-BigTech data and 2 epochs, tested 
with evaluation dataset:
Accuracy = 90,21%
Precision = 90,2%
Recall = 90,21%
F1 = 90,18%
tested with 4000 new rows of the same data:
Accuracy: 0.5525
Precision: 0.6684950730088312
Recall: 0.5525
F1-Score: 0.5044394096592938

BerTWEET-base-sentiment-analysis with Sentiment corpus and 2 epochs:
Accuracy: 0.8163
Precision: 0.8165
Recall: 0.8163
F1-Score: 0.8151

BERT-base-uncased fine-tuned using ~25k Tweets-Bigtech, 2 epochs, 
evaluated on separate test dataset:
Accuracy: 0.912
Precision: 0.9121157265185696
Recall: 0.912
F1-Score: 0.9118522077610691
evaluated on Brand-Sentiment-Analysis:
Accuracy: 0.5525151374010246
Precision: 0.6335749784291748
Recall: 0.5525151374010246
F1-Score: 0.5743773762349313

BERT-base-uncased fine-tuned with Sentiment corpus with 2 epochs,
evaluated on separate test dataset:
Accuracy: 0.8029
Precision: 0.8034
Recall: 0.8029
F1-Score: 0.8015


CardiffNLP Twitter RoBERTa base Sentiment latest, tested on 10k sample of sentiment corpus:
Accuracy: 0.6315
Precision: 0.6551255252262623
Recall: 0.6315
F1-Score: 0.6326440294565018

CardiffNLP Twitter RoBERTa base Sentiment latest after 2 epochs sentiment corpus, separate testset:
Accuracy: 0.8076
Precision: 0.8083
Recall: 0.8076
F1-Score: 0.8061 


Llama-2-7b-CHAT-hf without fine-tuning applied to Tweets-BigTech data (only on positive and negative posts):
Accuracy: 50,96%
Precision: 51,99%
Recall: 50,96%
F1-Score: 43,65%
after fine-tuning on 8k rows of Tweets-BigTech data:
Accuracy: 0.2831578947368421
Precision: 0.18883091733571217
Recall: 0.2831578947368421
F1-Score: 0.21714166747155744

Llama-2-7b-hf with fine-tuning on 8k rows of Tweets-BigTech data and 3 epochs:
Accuracy: 0.5636842105263158
Precision: 0.579594527089608
Recall: 0.5636842105263158
F1-Score: 0.5605039253912453

Llama-2-7b-hf with fine-tuning on 8k rows of Brand-Sentiment-Analysis-data and 2 epochs,
tested on Tweets-BigTech:
Accuracy: 0.5846530041918957
Precision: 0.5381664117219317
Recall: 0.5846530041918957
F1-Score: 0.5525406933042277

Llama-2-7b-hf with fine-tuning on 20k rows of Sentiment corpus and 2 epochs,
tested on separate test-split:
Accuracy: 0.7162
Precision: 0.7192
Recall: 0.7162
F1-Score: 0.7159



NER:

RoBERTa-Base-TweetNER7-all without additional fine-tuning on TweetNER7-data (NERvaluate):
Precision = 56,38%
Recall = 11,15%
F1 = 18,61%

RoBERTa-Base-TweetNER7-all without additional fine-tuning tested on TweetNER7-test-2021 (NERvaluate):
Precision = 68,51%
Recall = 68,29%
F1 = 68,4%
(evaluated with own function):
Precision: 0.9230
Recall: 0.9252
F1 Score: 0.9241

RoBERTa-Base-TweetNER7-all without additional fine-tuning tested on 10k rows of synthetic data (own evaluation):
Precision: 0.9744
Recall: 0.9525
F1 Score: 0.9634

RoBERTa-Base-TweetNER7-all without additional fine-tuning tested on 8k rows of WNUT (own evaluation):
Precision: 0.9703
Recall: 0.9349
F1 Score: 0.9523

RoBERTa-base with NER corpus, 3 epochs, trainer.evaluate:
Precision: 0.9534
Recall: 0.9553
F1 Score: 0.9543
NERvaluate:
ent_type:
Precision: 0.774188876013905 
Recall: 0.7594487070190395 
F1: 0.766747955816956
strict:
Precision: 0.7357 
Recall: 0.7217 
F1: 0.7286

BERT-base-cased fine-tuned with TweetNER7-big and 3 epochs (own evaluation), 
tested on "test2020"-part:
Precision: 0.6942
Recall: 0.8162
F1 Score: 0.7503

BERT-base-cased fine-tuned with TweetNER7-bigger and 3 epochs (left out and tested with 5% of data):
Precision: 0.8920
Recall: 0.9056
F1 Score: 0.8987

BERT-base-cased fine-tuned with 18k rows (after split) of NER-corpus, 2 epochs, tested on separate test-set (trainer.evaluate):
Precision: 0.9504
Recall: 0.9528
F1 Score: 0.9516
NERvaluate:
ent_type:
Precision: 0.7669094693028096 
Recall: 0.7330207445296959 
F1: 0.7495822738830366
strict:
Precision: 0.7262 
Recall: 0.6941 
F1: 0.7098


Covid-Twitter-BERT with 4700 rows of TweetNER7 and 3 epochs (NERvaluate): 
Precision = 47%
Recall = 5,3%
F1 = 9,5%

Covid-Twitter-BERT with 10000 rows of TweetNER7 and 5 epochs (NERvaluate): 
Precision = 33,1%
Recall = 3,6% 
F1 = 6,5%
increasing validation loss -> overfitting? bigger dataset necessary?

Covid-Twitter-BERT with 24k rows of NER-corpus, 2 epochs, separate testset:
trainer.evaluate:
Precision: 0.9548
Recall: 0.9571
F1 Score: 0.9554
NERvaluate:
ent_type: 
Precision: 0.7943
Recall: 0.7619
F1: 0.7777
strict: 
Precision: 0.6488
Recall: 0.6099
F1: 0.6288

Llama-2-7b-hf with 10000 rows of TweetNER7 and 2 epochs (with tweetner7-big, evaluated with trainer.evaluate/own evaluation function):
Precision = 80,21%
Recall = 86,38%
F1 = 81,87%
with 3 epochs:
Precision: 0.7995
Recall: 0.8643
F1 Score: 0.8306
with 4 epochs (tweetner7-bigger): 
Precision: 0.8788
Recall: 0.8985
F1 Score: 0.8886

Llama-2-7b-hf with 24k rows of NER corpus, 2 epochs, separate test dataset, 
simple evaluation:
Precision: 0.9304
Recall: 0.9342
F1 Score: 0.9323
NERvaluate:
ent_type:
Precision: 0.8294 
Recall: 0.5104 
F1: 0.6319
strict:
Precision: 0.7375
Recall: 0.4538
F1: 0.5619