{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openaikey = \"sk-proj-\"\n",
    "\n",
    "# setting it to the environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openaikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openaikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mCreate 1000 rows of synthetic tweet data for NER in CSV format.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mThe file should consist of a row for every token in the tweets.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mMake sure that the tweets make sense. Also only respond with the data.\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant designed to generate synthetic data.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m res \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Create 1000 rows of synthetic tweet data for NER in CSV format.\n",
    "The file should consist of a row for every token in the tweets.\n",
    "Each row should include the following fields:\n",
    " - token\n",
    " - label\n",
    " - sentence_id\n",
    "\n",
    "The token-column contains only one token for each row, the tokens put together should form concise social media posts.\n",
    "The label represents the entity of every token. Only label companies, products and persons. Every other token gets the value \"O\".\n",
    "The sentence_id increments every time a new sentence begins. It starts at 0. \n",
    "\n",
    "\n",
    "Make sure that the tweets make sense. Also only respond with the data.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    ")\n",
    "res = response.choices[0].message.content\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  \n",
    "        prompt=prompt,\n",
    "        max_tokens=100  \n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "prompt = \"Generate a twitter post that is suited .\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# templates for synthetic data generation\n",
    "import csv\n",
    "import random\n",
    "\n",
    "corporations = ['Apple', 'Google', 'Microsoft', 'Tesla', 'Amazon', 'Samsung', 'Blackberry', 'Nokia', 'Sony', 'Nintendo', 'Sega', 'OpenAI', 'Huawei']\n",
    "products = ['iPhone', 'iPad', 'Pixel', 'Windows 7', 'Model S', 'Echo', 'Galaxy', 'AirPods', 'AppleWatch', 'PlayStation', 'Xbox', 'GameCube', 'Fitwatch']\n",
    "persons = ['Elon', 'Musk', 'Tim', 'Cook', 'Steve Jobs', 'Jeff', 'Bezos', 'Sundar', 'Pichai', 'Smith', 'Miller', 'Davids', 'John', 'Richardson']\n",
    "\n",
    "tweet_templates = [\n",
    "    \"I love my new {product}, thanks {corporation}!\",\n",
    "    \"{person} just announced the new {product} from {corporation}.\",\n",
    "    \"The {product} from {corporation} is amazing!\",\n",
    "    \"{corporation}'s latest update to {product} is revolutionary.\",\n",
    "    \"{person} said that {corporation} is working on something big.\",\n",
    "    \"Have you seen the latest {corporation} event? {product} is the star!\",\n",
    "    \"{person} just bought a new {product}. Can't wait to try it!\",\n",
    "    \"Rumors say {corporation} is releasing {product} soon.\",\n",
    "    \"I hate the new {product}, the older ones are much better.\",\n",
    "    \"I just upgraded to the new {product}, {corporation} never disappoint!\",\n",
    "    \"{person} just hinted at new features in {corporation}'s upcoming {product}. I am hyped!\",\n",
    "    \"{person} is revolutionizing the industry with {corporation} and their {product}.\",\n",
    "    \"Just watched the {corporation} keynote. {product} looks impressive.\",\n",
    "    \"I'm curious, what do you think about {person}'s role at {corporation}?\",\n",
    "    \"Can not wait for {product} also. They should sell them down at SXSW.\",\n",
    "    \"The {corporation} store still has {product} and short lines.\",\n",
    "    \"more than 150 million mobile users for {product} for mobile #SXSW\",\n",
    "    \"Less than 2 hours until we announce the details on the {product} giveaway!\",\n",
    "    \"Is {corporation} going to drop another {product} soon? I need to know!\",\n",
    "    \"Looks like {person} is shaking things up at {corporation} with {product}.\",\n",
    "    \"{corporation} has a temporary Retail Store in Austin for the {product} release today. Opens at 5pm.\",\n",
    "    \"Reminder: {person} will be talking about {company} and {product} access today.\",\n",
    "    \"{corporation} is giving free {product} to open source coders who r attending this meet-up.\",\n",
    "    \"Is {person} still leading the innovation at {corporation} with {product}?\",\n",
    "    \"{corporation} just keeps raising the bar with every {product} they launch. Crazy!\",\n",
    "    \"{person} hyped up {corporation}'s {product}, but it's not that great in reality.\",\n",
    "    \"I am having so many issues with the {product}. {corporation} needs to fix this!\",\n",
    "    \"The innovation in {corporation}'s {product} is something only {person} could pull off.\",\n",
    "    \"Is it just me, or does {corporation}'s {product} feel rushed and unfinished?\",\n",
    "    \"Is it just me or does the new {product} feel rushed and unfinished? Disappointing\",\n",
    "    \"{corporation}'s {product} is overrated. I can't believe I fell for the marketing.\",\n",
    "    \"All eyes are on {corporation} after the announcement of their new {product}.\",\n",
    "    \"When {person} talks about {corporation}'s new {product}, you know it's going to be good.\",\n",
    "    \"The latest from {corporation}? Their {product} just dropped, and it's all over the internet.\",\n",
    "    \"I'm amazed at how {person} transformed {corporation} with innovations like {product}.\",\n",
    "    \"The {product} is making me rethink my loyalty to {corporation}. Its not good.\",\n",
    "    \"Honestly, {person} needs to focus on fixing {corporation}'s {product} before releasing new ones.\",\n",
    "    \"{person} teased some big changes for {corporation}'s {product}. I wonder what's next.\",\n",
    "    \"I trusted {person} to deliver a great {product} at {corporation}, but this is a flop.\",\n",
    "    \"{corporation} just came out with a new model, the {product}, which is electric. I love that!\",\n",
    "    \"{corporation}'s industry party tonight was great for the launch of {product}.\",\n",
    "    \"{product} is the must-have gadget of the year.\",\n",
    "    \"What is happening at {corporation}? {person} really needs to step up.\",\n",
    "    \"Who else is gonna get the new {product} next month?\",\n",
    "    \"It's time for {person} to leave {corporation}. What is he even doing.\",\n",
    "    \"{corporation} has been selected as the top AI startup in Austria, wow!\",\n",
    "    \"{person} was right! The {product} from {corporation} is revolutionary!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./synthetic_tweet_data3.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# creating the data with a row for each token and its label\n",
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "data = []\n",
    "sentence_id = 0\n",
    "\n",
    "while len(data) < 500000:\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "\n",
    "    tokens = tokenize(tweet)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        label = \"O\"\n",
    "        \n",
    "        if ' '.join(tokens[i:i + len(company.split())]) == company:\n",
    "            for j in range(len(company.split())):\n",
    "                data.append([tokens[i + j], \"B-company\" if j == 0 else \"I-company\", sentence_id])\n",
    "            i += len(company.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(product.split())]) == product:\n",
    "            for j in range(len(product.split())):\n",
    "                data.append([tokens[i + j], \"B-product\" if j == 0 else \"I-product\", sentence_id])\n",
    "            i += len(product.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(person.split())]) == person:\n",
    "            for j in range(len(person.split())):\n",
    "                data.append([tokens[i + j], \"B-person\" if j == 0 else \"I-person\", sentence_id])\n",
    "            i += len(person.split())\n",
    "            continue\n",
    "        else:\n",
    "            data.append([token, label, sentence_id])\n",
    "            i += 1\n",
    "    \n",
    "    sentence_id += 1\n",
    "\n",
    "csv_file_path = \"./synthetic_tweet_data3.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"token\", \"label\", \"sentence_id\"])\n",
    "    writer.writerows(data[:500000])\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./synthetic_tweet_data_grouped.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the data grouped, with one row per generated tweet\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "data = []\n",
    "\n",
    "while len(data) < 100000:\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "\n",
    "    tokens = tokenize(tweet)\n",
    "    labels = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        label = \"O\"\n",
    "        \n",
    "        if ' '.join(tokens[i:i + len(company.split())]) == company:\n",
    "            for j in range(len(company.split())):\n",
    "                labels.append(\"B-company\" if j == 0 else \"I-company\")\n",
    "            i += len(company.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(product.split())]) == product:\n",
    "            for j in range(len(product.split())):\n",
    "                labels.append(\"B-product\" if j == 0 else \"I-product\")\n",
    "            i += len(product.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(person.split())]) == person:\n",
    "            for j in range(len(person.split())):\n",
    "                labels.append(\"B-person\" if j == 0 else \"I-person\")\n",
    "            i += len(person.split())\n",
    "            continue\n",
    "        else:\n",
    "            labels.append(label)\n",
    "            i += 1\n",
    "\n",
    "    data.append([tokens, labels])\n",
    "\n",
    "csv_file_path = \"./synthetic_tweet_data_grouped.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"tokens\", \"labels\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 NER examples and saved to new_synthetic_ner_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# not necessary for final function to work\n",
    "def ensure_alignment(df):\n",
    "    for i, row in df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        tags = row['tags']\n",
    "        \n",
    "        if len(tokens) != len(tags):\n",
    "            print(f\"Fixing misalignment in row {i}: tokens={len(tokens)}, tags={len(tags)}\")\n",
    "            \n",
    "            if len(tokens) > len(tags):\n",
    "                tags.extend([\"O\"] * (len(tokens) - len(tags)))\n",
    "            elif len(tags) > len(tokens):\n",
    "                tags = tags[:len(tokens)]\n",
    "            \n",
    "            df.at[i, 'tags'] = tags\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = generate_ner_dataset(500)\n",
    "df = ensure_alignment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# working function but only in same notebook as df, used in ner deep learning\n",
    "def generate_ner_dataset(num_examples, output_file=\"new_synthetic_ner_dataset.csv\"):\n",
    "    \n",
    "    persons = [\n",
    "        \"John Smith\", \"Emily Chen\", \"Michael Johnson\", \"Sarah Williams\", \n",
    "        \"David Lee\", \"Maria Rodriguez\", \"James Brown\", \"Emma Davis\",\n",
    "        \"Robert Kim\", \"Jennifer Lopez\", \"Thomas Wilson\", \"Jessica Taylor\",\n",
    "        \"Carlos Vega\", \"Aisha Patel\", \"Daniel Park\", \"Olivia Nguyen\"\n",
    "    ]\n",
    "    \n",
    "    corporations = [\n",
    "        \"Google\", \"Microsoft\", \"Apple\", \"Amazon\", \"Meta\", \n",
    "        \"IBM\", \"Tesla\", \"Netflix\", \"Walmart\", \"JP Morgan\",\n",
    "        \"Acme Corp\", \"TechSolutions\", \"Global Systems\", \"DataWorks\",\n",
    "        \"Quantum Industries\", \"NexGen\", \"FutureSpace\", \"EcoSystems\"\n",
    "    ]\n",
    "    \n",
    "    products = [\n",
    "        \"iPhone 13\", \"Galaxy S22\", \"Surface Pro\", \"PlayStation 5\", \"Xbox Series X\",\n",
    "        \"MacBook Air\", \"Echo Dot\", \"AirPods Pro\", \"Tesla Model 3\", \"iPad Mini\",\n",
    "        \"Dyson V11\", \"Fitbit Charge\", \"Nintendo Switch\", \"Kindle Paperwhite\",\n",
    "        \"Roomba i7\", \"GoPro Hero\", \"Bose QuietComfort\", \"Instant Pot\"\n",
    "    ]\n",
    "    \n",
    "    events = [\n",
    "        \"CES 2023\", \"Web Summit\", \"SXSW\", \"TechCrunch Disrupt\", \"E3 Expo\",\n",
    "        \"Google I/O\", \"WWDC\", \"Consumer Electronics Show\", \"Mobile World Congress\",\n",
    "        \"Black Hat Conference\", \"DEF CON\", \"AWS re:Invent\", \"Game Developers Conference\",\n",
    "        \"Dreamforce\", \"Comic-Con\", \"Coachella\", \"New York Fashion Week\"\n",
    "    ]\n",
    "    \n",
    "    locations = [\n",
    "        \"New York\", \"San Francisco\", \"London\", \"Tokyo\", \"Berlin\",\n",
    "        \"Paris\", \"Sydney\", \"Toronto\", \"Chicago\", \"Seattle\", \n",
    "        \"Los Angeles\", \"Miami\", \"Singapore\", \"Hong Kong\",\n",
    "        \"Dubai\", \"Barcelona\", \"Austin\", \"Stockholm\", \"Seoul\"\n",
    "    ]\n",
    "    \n",
    "    templates = [\n",
    "        \"[PERSON] from [CORPORATION] announced that [PRODUCT] will be showcased at [EVENT] in [LOCATION].\",\n",
    "        \"At [EVENT], [PERSON] demonstrated how [PRODUCT] is revolutionizing [CORPORATION]'s approach in [LOCATION].\",\n",
    "        \"[CORPORATION] has selected [LOCATION] as the venue for [EVENT], where [PERSON] will launch [PRODUCT].\",\n",
    "        \"The new [PRODUCT] developed by [CORPORATION] will be presented by [PERSON] during [EVENT] in [LOCATION].\",\n",
    "        \"[PERSON] confirmed that [CORPORATION] will be expanding its [PRODUCT] line at this year's [EVENT] in [LOCATION].\",\n",
    "        \"According to [PERSON], [CORPORATION]'s latest [PRODUCT] has been well-received at [EVENT] in [LOCATION].\",\n",
    "        \"Reviews from [EVENT] suggest that [PERSON] made a strong case for [CORPORATION]'s new [PRODUCT] in the [LOCATION] market.\",\n",
    "        \"[CORPORATION] is planning to open a [PRODUCT] store in [LOCATION], announced [PERSON] at [EVENT].\",\n",
    "        \"The collaboration between [CORPORATION] and [PERSON] resulted in [PRODUCT], which will debut at [EVENT] in [LOCATION].\",\n",
    "        \"Attendees at [EVENT] in [LOCATION] were impressed when [PERSON] revealed [CORPORATION]'s innovative [PRODUCT].\",\n",
    "        \"[PERSON] traveled to [LOCATION] to promote [PRODUCT] at [EVENT] on behalf of [CORPORATION].\",\n",
    "        \"The [PRODUCT] team from [CORPORATION], led by [PERSON], won first prize at [EVENT] in [LOCATION].\",\n",
    "        \"Consumers in [LOCATION] can now purchase [PRODUCT] after [CORPORATION]'s expansion announcement by [PERSON] at [EVENT].\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        person = random.choice(persons)\n",
    "        corporation = random.choice(corporations)\n",
    "        product = random.choice(products)\n",
    "        event = random.choice(events)\n",
    "        location = random.choice(locations)\n",
    "        \n",
    "        sentence = template.replace(\"[PERSON]\", person)\\\n",
    "                          .replace(\"[CORPORATION]\", corporation)\\\n",
    "                          .replace(\"[PRODUCT]\", product)\\\n",
    "                          .replace(\"[EVENT]\", event)\\\n",
    "                          .replace(\"[LOCATION]\", location)\n",
    "        \n",
    "        tokens = []\n",
    "        tags = []\n",
    "        \n",
    "        raw_words = []\n",
    "        current_word = \"\"\n",
    "        for char in sentence:\n",
    "            if char.isalnum() or char in \"-'\":\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    raw_words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                if not char.isspace():\n",
    "                    raw_words.append(char)\n",
    "        if current_word:\n",
    "            raw_words.append(current_word)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(raw_words):\n",
    "            token = raw_words[i]\n",
    "            \n",
    "            found_entity = False\n",
    "            \n",
    "            if i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in persons:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-person\")\n",
    "                tags.append(\"I-person\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                tags.append(\"B-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                tags.append(\"B-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in locations:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-location\")\n",
    "                tags.append(\"I-location\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            if not found_entity:\n",
    "                if token in [name.split()[0] for name in persons]:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-person\")\n",
    "                    i += 1\n",
    "                elif token in corporations:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-corporation\")\n",
    "                    i += 1\n",
    "                elif token in products:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-product\")\n",
    "                    i += 1\n",
    "                elif token in events:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-event\")\n",
    "                    i += 1\n",
    "                elif token in locations:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-location\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"O\")\n",
    "                    i += 1\n",
    "        \n",
    "        data.append({\"tokens\": tokens, \"tags\": tags, \"sentence\": sentence})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# reworked new function\n",
    "def generate_ner_dataset(num_examples, output_file=\"new_synthetic_ner_dataset.csv\", templates=templates,\n",
    "                         persons=persons, corporations=corporations, products=products, events=events, \n",
    "                         locations=locations):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        person = random.choice(persons)\n",
    "        corporation = random.choice(corporations)\n",
    "        product = random.choice(products)\n",
    "        event = random.choice(events)\n",
    "        location = random.choice(locations)\n",
    "        \n",
    "        sentence = template.format(\n",
    "            person=person,\n",
    "            corporation=corporation,\n",
    "            product=product,\n",
    "            event=event,\n",
    "            location=location\n",
    "        )\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        \n",
    "        raw_words = []\n",
    "        current_word = \"\"\n",
    "        for char in sentence:\n",
    "            if char.isalnum() or char in \"-'\":\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    raw_words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                if not char.isspace():\n",
    "                    raw_words.append(char)\n",
    "        if current_word:\n",
    "            raw_words.append(current_word)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(raw_words):\n",
    "            token = raw_words[i]\n",
    "            \n",
    "            found_entity = False\n",
    "            \n",
    "            if i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in persons:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-person\")\n",
    "                labels.append(\"I-person\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in locations:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-location\")\n",
    "                labels.append(\"I-location\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            if not found_entity:\n",
    "                if token in [name.split()[0] for name in persons]:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-person\")\n",
    "                    i += 1\n",
    "                elif token in corporations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-corporation\")\n",
    "                    i += 1\n",
    "                elif token in products:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-product\")\n",
    "                    i += 1\n",
    "                elif token in events:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-event\")\n",
    "                    i += 1\n",
    "                elif token in locations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-location\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"O\")\n",
    "                    i += 1\n",
    "        \n",
    "        data.append({\"tokens\": tokens, \"labels\": labels, \"sentence\": sentence})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 NER examples and saved to new_synthetic_ner_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df = generate_ner_dataset(1000)\n",
    "df.to_csv(\"synthetic_ner_data_3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
