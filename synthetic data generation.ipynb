{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openaikey = \"sk-proj-\"\n",
    "\n",
    "# setting it to the environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openaikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openaikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Create 1000 rows of synthetic tweet data for NER in CSV format.\n",
    "The file should consist of a row for every token in the tweets.\n",
    "Each row should include the following fields:\n",
    " - token\n",
    " - label\n",
    " - sentence_id\n",
    "\n",
    "The token-column contains only one token for each row, the tokens put together should form concise social media posts.\n",
    "The label represents the entity of every token. Only label companies, products and persons. Every other token gets the value \"O\".\n",
    "The sentence_id increments every time a new sentence begins. It starts at 0. \n",
    "\n",
    "\n",
    "Make sure that the tweets make sense. Also only respond with the data.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    ")\n",
    "res = response.choices[0].message.content\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  \n",
    "        prompt=prompt,\n",
    "        max_tokens=100  \n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "prompt = \"Generate a twitter post that is suited .\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data generation with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# templates for synthetic data generation\n",
    "import csv\n",
    "import random\n",
    "\n",
    "corporations = ['Apple', 'Google', 'Microsoft', 'Tesla', 'Amazon', 'Samsung', 'Blackberry', 'Nokia', 'Sony', 'Nintendo', 'Sega', 'OpenAI', 'Huawei']\n",
    "products = ['iPhone', 'iPad', 'Pixel', 'Windows 7', 'Model S', 'Echo', 'Galaxy', 'AirPods', 'AppleWatch', 'PlayStation', 'Xbox', 'GameCube', 'Fitwatch']\n",
    "persons = ['Elon', 'Musk', 'Tim', 'Cook', 'Steve Jobs', 'Jeff', 'Bezos', 'Sundar', 'Pichai', 'Smith', 'Miller', 'Davids', 'John', 'Richardson']\n",
    "\n",
    "tweet_templates = [\n",
    "    \"I love my new {product}, thanks {corporation}!\",\n",
    "    \"{person} just announced the new {product} from {corporation}.\",\n",
    "    \"The {product} from {corporation} is amazing!\",\n",
    "    \"{corporation}'s latest update to {product} is revolutionary.\",\n",
    "    \"{person} said that {corporation} is working on something big.\",\n",
    "    \"Have you seen the latest {corporation} event? {product} is the star!\",\n",
    "    \"{person} just bought a new {product}. Can't wait to try it!\",\n",
    "    \"Rumors say {corporation} is releasing {product} soon.\",\n",
    "    \"I hate the new {product}, the older ones are much better.\",\n",
    "    \"I just upgraded to the new {product}, {corporation} never disappoint!\",\n",
    "    \"{person} just hinted at new features in {corporation}'s upcoming {product}. I am hyped!\",\n",
    "    \"{person} is revolutionizing the industry with {corporation} and their {product}.\",\n",
    "    \"Just watched the {corporation} keynote. {product} looks impressive.\",\n",
    "    \"I'm curious, what do you think about {person}'s role at {corporation}?\",\n",
    "    \"Can not wait for {product} also. They should sell them down at SXSW.\",\n",
    "    \"The {corporation} store still has {product} and short lines.\",\n",
    "    \"more than 150 million mobile users for {product} for mobile #SXSW\",\n",
    "    \"Less than 2 hours until we announce the details on the {product} giveaway!\",\n",
    "    \"Is {corporation} going to drop another {product} soon? I need to know!\",\n",
    "    \"Looks like {person} is shaking things up at {corporation} with {product}.\",\n",
    "    \"{corporation} has a temporary Retail Store in Austin for the {product} release today. Opens at 5pm.\",\n",
    "    \"Reminder: {person} will be talking about {company} and {product} access today.\",\n",
    "    \"{corporation} is giving free {product} to open source coders who r attending this meet-up.\",\n",
    "    \"Is {person} still leading the innovation at {corporation} with {product}?\",\n",
    "    \"{corporation} just keeps raising the bar with every {product} they launch. Crazy!\",\n",
    "    \"{person} hyped up {corporation}'s {product}, but it's not that great in reality.\",\n",
    "    \"I am having so many issues with the {product}. {corporation} needs to fix this!\",\n",
    "    \"The innovation in {corporation}'s {product} is something only {person} could pull off.\",\n",
    "    \"Is it just me, or does {corporation}'s {product} feel rushed and unfinished?\",\n",
    "    \"Is it just me or does the new {product} feel rushed and unfinished? Disappointing\",\n",
    "    \"{corporation}'s {product} is overrated. I can't believe I fell for the marketing.\",\n",
    "    \"All eyes are on {corporation} after the announcement of their new {product}.\",\n",
    "    \"When {person} talks about {corporation}'s new {product}, you know it's going to be good.\",\n",
    "    \"The latest from {corporation}? Their {product} just dropped, and it's all over the internet.\",\n",
    "    \"I'm amazed at how {person} transformed {corporation} with innovations like {product}.\",\n",
    "    \"The {product} is making me rethink my loyalty to {corporation}. Its not good.\",\n",
    "    \"Honestly, {person} needs to focus on fixing {corporation}'s {product} before releasing new ones.\",\n",
    "    \"{person} teased some big changes for {corporation}'s {product}. I wonder what's next.\",\n",
    "    \"I trusted {person} to deliver a great {product} at {corporation}, but this is a flop.\",\n",
    "    \"{corporation} just came out with a new model, the {product}, which is electric. I love that!\",\n",
    "    \"{corporation}'s industry party tonight was great for the launch of {product}.\",\n",
    "    \"{product} is the must-have gadget of the year.\",\n",
    "    \"What is happening at {corporation}? {person} really needs to step up.\",\n",
    "    \"Who else is gonna get the new {product} next month?\",\n",
    "    \"It's time for {person} to leave {corporation}. What is he even doing.\",\n",
    "    \"{corporation} has been selected as the top AI startup in Austria, wow!\",\n",
    "    \"{person} was right! The {product} from {corporation} is revolutionary!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data with a row for each token and its label\n",
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "data = []\n",
    "sentence_id = 0\n",
    "\n",
    "while len(data) < 500000:\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "\n",
    "    tokens = tokenize(tweet)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        label = \"O\"\n",
    "        \n",
    "        if ' '.join(tokens[i:i + len(company.split())]) == company:\n",
    "            for j in range(len(company.split())):\n",
    "                data.append([tokens[i + j], \"B-company\" if j == 0 else \"I-company\", sentence_id])\n",
    "            i += len(company.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(product.split())]) == product:\n",
    "            for j in range(len(product.split())):\n",
    "                data.append([tokens[i + j], \"B-product\" if j == 0 else \"I-product\", sentence_id])\n",
    "            i += len(product.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(person.split())]) == person:\n",
    "            for j in range(len(person.split())):\n",
    "                data.append([tokens[i + j], \"B-person\" if j == 0 else \"I-person\", sentence_id])\n",
    "            i += len(person.split())\n",
    "            continue\n",
    "        else:\n",
    "            data.append([token, label, sentence_id])\n",
    "            i += 1\n",
    "    \n",
    "    sentence_id += 1\n",
    "\n",
    "csv_file_path = \"./synthetic_tweet_data3.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"token\", \"label\", \"sentence_id\"])\n",
    "    writer.writerows(data[:500000])\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data grouped, with one row per generated tweet\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "data = []\n",
    "\n",
    "while len(data) < 100000:\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "\n",
    "    tokens = tokenize(tweet)\n",
    "    labels = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        label = \"O\"\n",
    "        \n",
    "        if ' '.join(tokens[i:i + len(company.split())]) == company:\n",
    "            for j in range(len(company.split())):\n",
    "                labels.append(\"B-company\" if j == 0 else \"I-company\")\n",
    "            i += len(company.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(product.split())]) == product:\n",
    "            for j in range(len(product.split())):\n",
    "                labels.append(\"B-product\" if j == 0 else \"I-product\")\n",
    "            i += len(product.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(person.split())]) == person:\n",
    "            for j in range(len(person.split())):\n",
    "                labels.append(\"B-person\" if j == 0 else \"I-person\")\n",
    "            i += len(person.split())\n",
    "            continue\n",
    "        else:\n",
    "            labels.append(label)\n",
    "            i += 1\n",
    "\n",
    "    data.append([tokens, labels])\n",
    "\n",
    "csv_file_path = \"./synthetic_tweet_data_grouped.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"tokens\", \"labels\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary for final function to work\n",
    "def ensure_alignment(df):\n",
    "    for i, row in df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        tags = row['tags']\n",
    "        \n",
    "        if len(tokens) != len(tags):\n",
    "            print(f\"Fixing misalignment in row {i}: tokens={len(tokens)}, tags={len(tags)}\")\n",
    "            \n",
    "            if len(tokens) > len(tags):\n",
    "                tags.extend([\"O\"] * (len(tokens) - len(tags)))\n",
    "            elif len(tags) > len(tokens):\n",
    "                tags = tags[:len(tokens)]\n",
    "            \n",
    "            df.at[i, 'tags'] = tags\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = generate_ner_dataset(500)\n",
    "df = ensure_alignment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# working function but only in same notebook as df, used in ner deep learning\n",
    "def generate_ner_dataset(num_examples, output_file=\"new_synthetic_ner_dataset.csv\"):\n",
    "    \n",
    "    persons = [\n",
    "        \"John Smith\", \"Emily Chen\", \"Michael Johnson\", \"Sarah Williams\", \n",
    "        \"David Lee\", \"Maria Rodriguez\", \"James Brown\", \"Emma Davis\",\n",
    "        \"Robert Kim\", \"Jennifer Lopez\", \"Thomas Wilson\", \"Jessica Taylor\",\n",
    "        \"Carlos Vega\", \"Aisha Patel\", \"Daniel Park\", \"Olivia Nguyen\"\n",
    "    ]\n",
    "    \n",
    "    corporations = [\n",
    "        \"Google\", \"Microsoft\", \"Apple\", \"Amazon\", \"Meta\", \n",
    "        \"IBM\", \"Tesla\", \"Netflix\", \"Walmart\", \"JP Morgan\",\n",
    "        \"Acme Corp\", \"TechSolutions\", \"Global Systems\", \"DataWorks\",\n",
    "        \"Quantum Industries\", \"NexGen\", \"FutureSpace\", \"EcoSystems\"\n",
    "    ]\n",
    "    \n",
    "    products = [\n",
    "        \"iPhone 13\", \"Galaxy S22\", \"Surface Pro\", \"PlayStation 5\", \"Xbox Series X\",\n",
    "        \"MacBook Air\", \"Echo Dot\", \"AirPods Pro\", \"Tesla Model 3\", \"iPad Mini\",\n",
    "        \"Dyson V11\", \"Fitbit Charge\", \"Nintendo Switch\", \"Kindle Paperwhite\",\n",
    "        \"Roomba i7\", \"GoPro Hero\", \"Bose QuietComfort\", \"Instant Pot\"\n",
    "    ]\n",
    "    \n",
    "    events = [\n",
    "        \"CES 2023\", \"Web Summit\", \"SXSW\", \"TechCrunch Disrupt\", \"E3 Expo\",\n",
    "        \"Google I/O\", \"WWDC\", \"Consumer Electronics Show\", \"Mobile World Congress\",\n",
    "        \"Black Hat Conference\", \"DEF CON\", \"AWS re:Invent\", \"Game Developers Conference\",\n",
    "        \"Dreamforce\", \"Comic-Con\", \"Coachella\", \"New York Fashion Week\"\n",
    "    ]\n",
    "    \n",
    "    locations = [\n",
    "        \"New York\", \"San Francisco\", \"London\", \"Tokyo\", \"Berlin\",\n",
    "        \"Paris\", \"Sydney\", \"Toronto\", \"Chicago\", \"Seattle\", \n",
    "        \"Los Angeles\", \"Miami\", \"Singapore\", \"Hong Kong\",\n",
    "        \"Dubai\", \"Barcelona\", \"Austin\", \"Stockholm\", \"Seoul\"\n",
    "    ]\n",
    "    \n",
    "    templates = [\n",
    "        \"[PERSON] from [CORPORATION] announced that [PRODUCT] will be showcased at [EVENT] in [LOCATION].\",\n",
    "        \"At [EVENT], [PERSON] demonstrated how [PRODUCT] is revolutionizing [CORPORATION]'s approach in [LOCATION].\",\n",
    "        \"[CORPORATION] has selected [LOCATION] as the venue for [EVENT], where [PERSON] will launch [PRODUCT].\",\n",
    "        \"The new [PRODUCT] developed by [CORPORATION] will be presented by [PERSON] during [EVENT] in [LOCATION].\",\n",
    "        \"[PERSON] confirmed that [CORPORATION] will be expanding its [PRODUCT] line at this year's [EVENT] in [LOCATION].\",\n",
    "        \"According to [PERSON], [CORPORATION]'s latest [PRODUCT] has been well-received at [EVENT] in [LOCATION].\",\n",
    "        \"Reviews from [EVENT] suggest that [PERSON] made a strong case for [CORPORATION]'s new [PRODUCT] in the [LOCATION] market.\",\n",
    "        \"[CORPORATION] is planning to open a [PRODUCT] store in [LOCATION], announced [PERSON] at [EVENT].\",\n",
    "        \"The collaboration between [CORPORATION] and [PERSON] resulted in [PRODUCT], which will debut at [EVENT] in [LOCATION].\",\n",
    "        \"Attendees at [EVENT] in [LOCATION] were impressed when [PERSON] revealed [CORPORATION]'s innovative [PRODUCT].\",\n",
    "        \"[PERSON] traveled to [LOCATION] to promote [PRODUCT] at [EVENT] on behalf of [CORPORATION].\",\n",
    "        \"The [PRODUCT] team from [CORPORATION], led by [PERSON], won first prize at [EVENT] in [LOCATION].\",\n",
    "        \"Consumers in [LOCATION] can now purchase [PRODUCT] after [CORPORATION]'s expansion announcement by [PERSON] at [EVENT].\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        person = random.choice(persons)\n",
    "        corporation = random.choice(corporations)\n",
    "        product = random.choice(products)\n",
    "        event = random.choice(events)\n",
    "        location = random.choice(locations)\n",
    "        \n",
    "        sentence = template.replace(\"[PERSON]\", person)\\\n",
    "                          .replace(\"[CORPORATION]\", corporation)\\\n",
    "                          .replace(\"[PRODUCT]\", product)\\\n",
    "                          .replace(\"[EVENT]\", event)\\\n",
    "                          .replace(\"[LOCATION]\", location)\n",
    "        \n",
    "        tokens = []\n",
    "        tags = []\n",
    "        \n",
    "        raw_words = []\n",
    "        current_word = \"\"\n",
    "        for char in sentence:\n",
    "            if char.isalnum() or char in \"-'\":\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    raw_words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                if not char.isspace():\n",
    "                    raw_words.append(char)\n",
    "        if current_word:\n",
    "            raw_words.append(current_word)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(raw_words):\n",
    "            token = raw_words[i]\n",
    "            \n",
    "            found_entity = False\n",
    "            \n",
    "            if i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in persons:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-person\")\n",
    "                tags.append(\"I-person\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                tags.append(\"B-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-product\")\n",
    "                tags.append(\"I-product\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                tags.append(\"B-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-event\")\n",
    "                tags.append(\"I-event\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in locations:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tags.append(\"B-location\")\n",
    "                tags.append(\"I-location\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            if not found_entity:\n",
    "                if token in [name.split()[0] for name in persons]:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-person\")\n",
    "                    i += 1\n",
    "                elif token in corporations:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-corporation\")\n",
    "                    i += 1\n",
    "                elif token in products:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-product\")\n",
    "                    i += 1\n",
    "                elif token in events:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-event\")\n",
    "                    i += 1\n",
    "                elif token in locations:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"B-location\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    tags.append(\"O\")\n",
    "                    i += 1\n",
    "        \n",
    "        data.append({\"tokens\": tokens, \"tags\": tags, \"sentence\": sentence})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# reworked new function\n",
    "def generate_ner_dataset(num_examples, output_file=\"new_synthetic_ner_dataset.csv\", templates=templates,\n",
    "                         persons=persons, corporations=corporations, products=products, events=events, \n",
    "                         locations=locations):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        person = random.choice(persons)\n",
    "        corporation = random.choice(corporations)\n",
    "        product = random.choice(products)\n",
    "        event = random.choice(events)\n",
    "        location = random.choice(locations)\n",
    "        \n",
    "        sentence = template.format(\n",
    "            person=person,\n",
    "            corporation=corporation,\n",
    "            product=product,\n",
    "            event=event,\n",
    "            location=location\n",
    "        )\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        \n",
    "        raw_words = []\n",
    "        current_word = \"\"\n",
    "        for char in sentence:\n",
    "            if char.isalnum() or char in \"-'\":\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    raw_words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                if not char.isspace():\n",
    "                    raw_words.append(char)\n",
    "        if current_word:\n",
    "            raw_words.append(current_word)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(raw_words):\n",
    "            token = raw_words[i]\n",
    "            \n",
    "            found_entity = False\n",
    "            \n",
    "            if i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in persons:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-person\")\n",
    "                labels.append(\"I-person\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in locations:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-location\")\n",
    "                labels.append(\"I-location\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            if not found_entity:\n",
    "                if token in [name.split()[0] for name in persons]:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-person\")\n",
    "                    i += 1\n",
    "                elif token in corporations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-corporation\")\n",
    "                    i += 1\n",
    "                elif token in products:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-product\")\n",
    "                    i += 1\n",
    "                elif token in events:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-event\")\n",
    "                    i += 1\n",
    "                elif token in locations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-location\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"O\")\n",
    "                    i += 1\n",
    "        \n",
    "        data.append({\"tokens\": tokens, \"labels\": labels, \"sentence\": sentence})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_ner_dataset(1000)\n",
    "df.to_csv(\"synthetic_ner_data_3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
