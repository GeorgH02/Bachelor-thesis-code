{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data analysis and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just tweets mentioning Dell during first three quarters of 2022 \n",
    "# https://www.kaggle.com/datasets/ankitkumar2635/dell-tweets-2022\n",
    "import pandas as pd\n",
    "\n",
    "df_dell = pd.read_csv(\"./analysis_data/First three qtr Dell tweets.csv\")\n",
    "df_dell.dropna(inplace=True)\n",
    "df_dell.drop_duplicates(inplace=True)\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# counting most frequent mentions\n",
    "df_dell[\"mentions\"] = df_dell[\"Text\"].apply(lambda x: re.findall(r'@\\S+', str(x)) if pd.notna(x) else [])\n",
    "all_mentions = [mention for mentions in df_dell[\"mentions\"] for mention in mentions]\n",
    "mention_counts = pd.Series(Counter(all_mentions))\n",
    "mention_counts = mention_counts.sort_values(ascending=False)\n",
    "for index, value in mention_counts[:100].items():\n",
    "    print(f\"{index}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing urls and user mentions while keeping select mentions\n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "\n",
    "mentions_to_keep = [\"Dell\", \"Delltech\", \"Dellcares\", \"Dell,\", \"DellOutlet\", \"Dell_IN\", \"DellCaresPRO\", \"DellTechIndia\",\n",
    "                    \"Dell's\", \"DellServices\", \"dellindia\", \"DellUK\", \"DellTechZA\", \"FedEx\", \"HPIndia\", \"PwC\", \"Delta\",\n",
    "                    \"Deloitte\", \"AlienwareTech\", \"AMD\", \"Acer\", \"Tesla\", \"comcast\", \"Meta\", \"nvidia\", \"Ford\", \"YouTube\",\n",
    "                    \"HP\", \"Microsoft\", \"Apple\", \"Logitech\", \"ubuntu\", \"playstation\", \"Windows\", \"Oracle\", \"intelcanada\",\n",
    "                    \"Google\", \"ASUS\", \"Lenovo\", \"Tesla\", \"Intel\", \"Alienware\", \"Emc\", \"WWEUniverse\", \"AmericanAir\",\n",
    "                    \"Zipchair\", \"ZipchairGaming\", \"RapSnacksNow\", \"ATT\", \"IBM\", \"amazon\", \"Walmart\", \"Cisco\", \n",
    "                    \"MichaelDell\", \"AlokOhrie\", \"Elonmusk\", \"JohnCena\", \"TheRock\", \"steveaustinBSR\", \"_KennyRogers\",\n",
    "                    \"VMware\", \"SouthwestAir\", \"Lenovo_in\"]\n",
    "\n",
    "extractor = URLExtract()\n",
    "\n",
    "def format_tweet_new_keep(tweet):\n",
    "\n",
    "    tokens = tweet.split()\n",
    "    tokens = [token[1:].capitalize() if token.startswith(\"@\") and token[1:].capitalize() in mentions_to_keep else token for token in tokens]\n",
    "\n",
    "    tweet = \" \".join(tokens)\n",
    "\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, \"{{URL}}\")\n",
    "\n",
    "    tweet = re.sub(r'@\\S+', \"{{MENTION}}\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "df_dell[\"text\"] = df_dell.apply(lambda x: format_tweet_new_keep(str(x[\"Text\"])), axis=1)\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell[\"tokens\"] = df_dell[\"text\"].apply(lambda x: x.split())\n",
    "df_dell[\"tokens\"].str.len().agg([\"mean\",\"max\",\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell = df_dell[[\"Datetime\", \"text\", \"tokens\"]]\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell.to_json(\"./analysis_data/dell_cs_processed.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting entity- and sentiment-labels is performed in the respective notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell_p = pd.read_json(\"./analysis_data/dell_cs_after_predictions.json\", orient=\"records\")\n",
    "df_dell_p[\"sentiment_labels\"] = df_dell_p[\"bert_pred_sa\"].replace({0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"})\n",
    "df_dell_p[\"Datetime\"] = pd.to_datetime(df_dell_p[\"Datetime\"])\n",
    "df_dell_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_dell_p[\"tokens\"].str.len() != df_dell_p[\"covid_bert_pred\"].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the rows with mismatched lengths (caused by truncation to max length of 128 tokens)\n",
    "df_dell_p = df_dell_p[df_dell_p[\"tokens\"].str.len() == df_dell_p[\"covid_bert_pred\"].str.len()]\n",
    "print(f\"Number of rows predicted in their entirety: {len(df_dell_p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell_p[df_dell_p[\"bert_pred_sa\"].isin([0, 1, 2])].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dell_p[\"sentiment_labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the tags of recognized instances of Dell mentions from tag-column to 10 (\"O\"-tag)\n",
    "own_company_names = [\"Dell\", \"Delltech\", \"Dellcares\", \"Dell,\", \"DellOutlet\", \"Dell_IN\", \"DellCaresPRO\", \"DellTechIndia\",\n",
    "                    \"Dell's\", \"DellServices\", \"dellindia\", \"DellUK\", \"DellTechZA\"]\n",
    "\n",
    "for index, row in df_dell_p.iterrows():\n",
    "    tokens = row[\"tokens\"]\n",
    "    labels = row[\"covid_bert_pred\"]\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in own_company_names and labels[i] != 10:\n",
    "            labels[i] = 10\n",
    "            modified = True\n",
    "    \n",
    "    if modified:\n",
    "        df_dell_p.at[index, \"covid_bert_pred\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    0: \"B-corporation\",\n",
    "    1: \"B-event\",\n",
    "    2: \"B-location\",\n",
    "    3: \"B-person\",\n",
    "    4: \"B-product\",\n",
    "    5: \"I-corporation\",\n",
    "    6: \"I-event\",\n",
    "    7: \"I-location\",\n",
    "    8: \"I-person\",\n",
    "    9: \"I-product\",\n",
    "    10: \"O\"\n",
    "}\n",
    "\n",
    "label_list = list(entity_dict.values())\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "df_dell_p[\"ner_labels\"] = df_dell_p[\"covid_bert_pred\"].apply(lambda x: [id_to_label[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types_from_list(lst):\n",
    "    r = []\n",
    "    for entry in lst:\n",
    "        if entry != \"O\":\n",
    "            parts = entry.split(\"-\", 1)\n",
    "            if len(parts) > 1:\n",
    "                entity_type = parts[1]\n",
    "                if entity_type not in r:\n",
    "                    r.append(entity_type)\n",
    "    return r\n",
    "\n",
    "df_dell_p[\"entity_types\"] = df_dell_p[\"ner_labels\"].apply(get_types_from_list)\n",
    "df_dell_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell_p[\"entity_types\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visual explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique_entities = [\"corporation\", \"product\", \"person\", \"location\", \"event\"]\n",
    "\n",
    "def mentioned_entities_with_sentiment_breakdown(df_results):\n",
    "    dic = {}\n",
    "    for entity_type in unique_entities:\n",
    "        df_results[\"entity_mentioned\"] = df_results[\"entity_types\"].apply(lambda x: entity_type in x)\n",
    "        filtered_df = df_results[df_results[\"entity_mentioned\"]==True]\n",
    "        percentage = len(filtered_df)/len(df_results)\n",
    "        dic[entity_type] = percentage\n",
    "\n",
    "    print(dic)\n",
    "\n",
    "    entity_dic = {}\n",
    "    for entity_type in unique_entities:\n",
    "        df_results[\"entity_mentioned\"] = df_results[\"entity_types\"].apply(lambda x: entity_type in x)\n",
    "        filtered_df = df_results[df_results[\"entity_mentioned\"]==True]\n",
    "        percentage = len(filtered_df)/len(df_results)\n",
    "        entity_dic[entity_type] = percentage\n",
    "\n",
    "    sentiment_dic = {}\n",
    "    for entity_type in unique_entities:\n",
    "        df_results[\"entity_mentioned\"] = df_results[\"entity_types\"].apply(lambda x: entity_type in x)\n",
    "        filtered_df = df_results[df_results[\"entity_mentioned\"]==True]\n",
    "        sentiment_dic[entity_type] = {\"negative\": 0, \"neutral\": 0, \"positive\": 0}\n",
    "\n",
    "        total_count = len(filtered_df)\n",
    "        negative_count = len(filtered_df[filtered_df[\"sentiment_labels\"] == \"negative\"])\n",
    "        neutral_count = len(filtered_df[filtered_df[\"sentiment_labels\"] == \"neutral\"])\n",
    "        positive_count = len(filtered_df[filtered_df[\"sentiment_labels\"] == \"positive\"])\n",
    "        \n",
    "        sentiment_dic[entity_type] = {\n",
    "            \"negative\": (negative_count / total_count) * dic[entity_type],\n",
    "            \"neutral\": (neutral_count / total_count) * dic[entity_type],\n",
    "            \"positive\": (positive_count / total_count) * dic[entity_type]\n",
    "        }\n",
    "        \n",
    "    entity_types = list(entity_dic.keys())\n",
    "    x = range(len(entity_types))\n",
    "    total_percentages = list(entity_dic.values())\n",
    "\n",
    "    negative_values = [sentiment_dic[entity][\"negative\"] for entity in entity_types]\n",
    "    neutral_values = [sentiment_dic[entity][\"neutral\"] for entity in entity_types]\n",
    "    positive_values = [sentiment_dic[entity][\"positive\"] for entity in entity_types]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    p1 = ax.bar(x, negative_values, align=\"center\", label=\"Negative\", color=\"blue\")\n",
    "    p2 = ax.bar(x, neutral_values, align=\"center\", bottom=negative_values, label=\"Neutral\", color=\"orange\")\n",
    "    p3 = ax.bar(x, positive_values, align=\"center\", \n",
    "                bottom=[p+n for p, n in zip(negative_values, neutral_values)], \n",
    "                label=\"Positive\", color=\"green\")\n",
    "\n",
    "    for i, percentage in enumerate(total_percentages):\n",
    "        v = f\"{round(percentage*100, 2)}%\"\n",
    "        ax.text(i, percentage, v, ha=\"center\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(entity_types)\n",
    "    ax.set_title(\"Percentage of postings mentioning the entity type with sentiment breakdown\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "mentioned_entities_with_sentiment_breakdown(df_dell_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusions: \n",
    "- corporations and products are mentioned the most, in over 35% of all posts\n",
    "- of corporation- and product-posts, negativity is the most common sentiment\n",
    "- the large majority of posts contain sentiment, neutrality is quite rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating wordclouds to investigate the vocabulary of negative corporation- and product posts\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#adding frequently mentioned uninteresting words to the stopwords to not be included in the wordcloud\n",
    "stopwords.update([\"mention\", \"rt\", \"link\", \"url\", \"quot\", \"dell\", \"dellcares\", \"dellcare\", \"delltech\"])\n",
    "\n",
    "def create_wordcloud(df, entity, sentiment, text_col=\"text\"):\n",
    "    words = \"\"\n",
    "\n",
    "    df[\"entity_mentioned\"] = df[\"entity_types\"].apply(lambda x: entity in x)\n",
    "    filtered_df = df[(df[\"sentiment_labels\"]==sentiment) & (df[\"entity_mentioned\"]==True)]\n",
    "\n",
    "    for val in filtered_df[text_col]:\n",
    "        val = str(val)\n",
    "        tokens = val.split()\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "        \n",
    "        words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=800,\n",
    "                        background_color=\"white\",\n",
    "                        stopwords=stopwords,\n",
    "                        min_font_size=10).generate(words)\n",
    "                    \n",
    "    plt.figure(figsize = (5, 5), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.title(f\"Wordcloud of all {sentiment} postings mentioning a {entity}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_dell_p, \"corporation\", \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_dell_p, \"corporation\", \"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_dell_p, \"product\", \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df_dell_p, \"product\", \"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusions:\n",
    "- corporation: general problems related to technology, sometimes related to competitors\n",
    "- product: warranty often related, apparently mostly general issues with products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity type pair sentiment heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "all_entity_types = sorted(set([entity for sublist in df_dell_p['entity_types'] for entity in sublist]))\n",
    "\n",
    "sentiment_matrix = pd.DataFrame(\n",
    "    index=all_entity_types,\n",
    "    columns=all_entity_types,\n",
    "    dtype=float\n",
    ")\n",
    "\n",
    "for i, type1 in enumerate(all_entity_types):\n",
    "    for j, type2 in enumerate(all_entity_types):\n",
    "        if i <= j:\n",
    "            mask = df_dell_p['entity_types'].apply(lambda x: type1 in x and type2 in x)\n",
    "            if mask.sum() > 0:\n",
    "                avg_sentiment = df_dell_p.loc[mask, 'bert_pred_sa'].mean()\n",
    "                sentiment_matrix.loc[type1, type2] = avg_sentiment\n",
    "                if i != j:  \n",
    "                    sentiment_matrix.loc[type2, type1] = avg_sentiment\n",
    "\n",
    "# adjusting the order of the y-axis\n",
    "sentiment_matrix_aligned = sentiment_matrix.iloc[::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(\n",
    "    sentiment_matrix_aligned,  \n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=plt.cm.RdYlGn,\n",
    "    center=1,\n",
    "    cbar_kws={'label': 'Average Sentiment Score'}\n",
    ")\n",
    "plt.title(\"Entity Type Pair Sentiment Heatmap\")\n",
    "plt.xlabel(\"Entity Type\")\n",
    "plt.ylabel(\"Entity Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusions:\n",
    "- events mentioned in positive discussions regardless of other entities present\n",
    "- location-person the next strongest pairing, persons in general also quite positive\n",
    "- product-corporation is the most negatively associated pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 10 instances with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting instance based insight to investigate most frequently mentioned instances of entities\n",
    "import regex as re\n",
    "\n",
    "instances_to_remove = own_company_names + [\"Mention\", \"Url\", \"Computer\", \"Laptop\", \"Laptops\", \"Pc\", \"Monitor\", \"2022\"]\n",
    "\n",
    "entity_mapping = {\n",
    "    \"person\": {\n",
    "        \"Elon\": \"Elon Musk\",\n",
    "        \"Elonmusk\": \"Elon Musk\",\n",
    "        \"Elon musk\": \"Elon Musk\"\n",
    "    },\n",
    "    \"location\": {\n",
    "        \"Tx\" : \"Texas\",\n",
    "        \"Us\" : \"Usa\"\n",
    "    },\n",
    "    \"event\": {\n",
    "        \"Ces\" : \"Ces 2022\",\n",
    "        \"New year\" : \"New years\" \n",
    "    },\n",
    "    \"corporation\": {\n",
    "         \"HPIndia\" : \"HP\",\n",
    "         \"Lenovo_in\" : \"Lenovo\",\n",
    "         \"intelcanada\" : \"Intel\"\n",
    "    }\n",
    "#         \"Dell computers\": \"Dell\",\n",
    "#         \"Dell tech\": \"Dell Technologies\",\n",
    "#         \"Dell technology\": \"Dell Technologies\",\n",
    "#     },\n",
    "#     \"product\": {\n",
    "#         \"Xps13\": \"XPS 13\",\n",
    "#         \"Xps 13plus\": \"XPS 13 Plus\",\n",
    "#         \"Alienware\": \"Alienware laptop\",\n",
    "#     }\n",
    "}\n",
    "\n",
    "def get_instance_dic(df):\n",
    "    unique_entities = [\"corporation\", \"product\", \"person\", \"location\", \"event\"]\n",
    "    instance_dic = {}\n",
    "    \n",
    "    for type in unique_entities:\n",
    "        type_list = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            tokens = row[\"tokens\"]\n",
    "            labels = row[\"ner_labels\"]\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(labels):\n",
    "                if labels[i] == f\"B-{type}\":\n",
    "                    entity_tokens = [tokens[i]]\n",
    "                    j = i + 1\n",
    "                    \n",
    "                    while j < len(labels) and labels[j] == f\"I-{type}\":\n",
    "                        entity_tokens.append(tokens[j])\n",
    "                        j += 1\n",
    "                    \n",
    "                    complete_entity = \" \".join(entity_tokens)\n",
    "                    complete_entity = re.sub(r'[^a-zA-Z0-9\\s]', '', complete_entity).capitalize().strip().removesuffix(\"'s\")\n",
    "                    \n",
    "                    # applying manual mapping to group select instances together\n",
    "                    if type in entity_mapping and complete_entity in entity_mapping[type]:\n",
    "                        complete_entity = entity_mapping[type][complete_entity]\n",
    "                    \n",
    "                    if complete_entity not in instances_to_remove:\n",
    "                        type_list.append(complete_entity)\n",
    "                    \n",
    "                    i = j\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        instance_dic[type] = type_list\n",
    "    \n",
    "     # remove trailing \"s\" if instance is already known with 2nd pass through instance_dic to replace \"Googles\" with \"Google\"\n",
    "    for type in instance_dic:\n",
    "        unique_instances = set(instance_dic[type])  \n",
    "        new_type_list = []\n",
    "\n",
    "        for entity in instance_dic[type]:\n",
    "            if entity.endswith(\"s\"):\n",
    "                singular_candidate = entity[:-1]  \n",
    "                if singular_candidate in unique_instances:\n",
    "                    entity = singular_candidate \n",
    "            \n",
    "            new_type_list.append(entity)\n",
    "\n",
    "        instance_dic[type] = new_type_list\n",
    "\n",
    "    return instance_dic\n",
    "\n",
    "instance_dic = get_instance_dic(df_dell_p)\n",
    "print(instance_dic[\"product\"])\n",
    "    \n",
    "for key in instance_dic.keys():\n",
    "    print(f\"Detected instances of {key}: {len(instance_dic[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_instances(instance_dic, top_n=10):\n",
    "    top_instances = {}\n",
    "    \n",
    "    for entity_type, instances in instance_dic.items():\n",
    "        instance_counts = {}\n",
    "        for instance in instances:\n",
    "            instance_counts[instance] = instance_counts.get(instance, 0) + 1\n",
    "        \n",
    "        sorted_instances = sorted(instance_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_n_instances = sorted_instances[:top_n]\n",
    "        \n",
    "        top_instances[entity_type] = top_n_instances\n",
    "\n",
    "    return top_instances\n",
    "\n",
    "top_10_instances = get_top_instances(instance_dic, top_n=10)\n",
    "for key in top_10_instances.keys():\n",
    "    print(f\"{key}: {top_10_instances[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding sentiment to the entity-instances, not including instances that could not be found in the text-column\n",
    "def add_sentiment_to_instances(entity_dict, df):\n",
    "    result = {}\n",
    "    \n",
    "    for entity_type, instances in entity_dict.items():\n",
    "        instances_with_sentiment = []\n",
    "        instances_not_found = []\n",
    "        \n",
    "        for entity, count in instances:\n",
    "            entity_lower = entity.lower()\n",
    "            \n",
    "            # filtering in text-column to detect multi-token instances\n",
    "            mask = df[\"text\"].apply(lambda x: entity_lower in x.lower())\n",
    "            \n",
    "            if mask.sum() > 0:\n",
    "                avg_sentiment = df.loc[mask, \"bert_pred_sa\"].mean()\n",
    "                instances_with_sentiment.append((entity, count, avg_sentiment))\n",
    "            else:\n",
    "                instances_not_found.append(entity)\n",
    "        \n",
    "        if instances_not_found:\n",
    "            print(f\"{len(instances_not_found)} {entity_type} entities not found in text: {instances_not_found}\")\n",
    "            \n",
    "        result[entity_type] = instances_with_sentiment\n",
    "    \n",
    "    return result\n",
    "\n",
    "top_10_instances2 = add_sentiment_to_instances(top_10_instances, df_dell_p)\n",
    "for key in top_10_instances2.keys():\n",
    "    print(f\"{key}: {top_10_instances2[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "def barchart_top10_instances_with_sentiment(entity):\n",
    "    instances = [item[0] for item in top_10_instances2[entity]]\n",
    "    counts = [item[1] for item in top_10_instances2[entity]]\n",
    "    sentiments = [item[2] for item in top_10_instances2[entity]]\n",
    "\n",
    "    cmap = plt.cm.RdYlGn  \n",
    "    norm = mcolors.Normalize(vmin=0, vmax=2) # color-range from 0 to 2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(range(len(instances)), counts, align=\"center\")\n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        if np.isnan(sentiments[i]):\n",
    "            bar.set_color(\"black\")  \n",
    "        else:\n",
    "            bar_color = cmap(norm(sentiments[i]))\n",
    "            bar.set_color(bar_color)\n",
    "\n",
    "    for i, v in enumerate(counts):\n",
    "        ax.text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "    for i, sentiment in enumerate(sentiments):\n",
    "        ax.text(i, -5, f\"({sentiment:.2f})\", ha='center', va='top', color='blue')\n",
    "\n",
    "    ax.set_xticks(range(len(instances)))\n",
    "    ax.set_xticklabels(instances, rotation=45, ha='right')\n",
    "    ax.set_xlabel(f\"{entity.capitalize()}s\")\n",
    "    ax.set_ylabel(f\"Posts mentioning this {entity}\")\n",
    "    ax.set_title(f\"Top 10 most frequently mentioned {entity}s (colored by sentiment)\")\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Sentiment Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "barchart_top10_instances_with_sentiment(\"product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart_top10_instances_with_sentiment(\"corporation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart_top10_instances_with_sentiment(\"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart_top10_instances_with_sentiment(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart_top10_instances_with_sentiment(\"event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusions:\n",
    "- corporations: posts mentioning \"intel\" are the most positive out of the top 10 instances, with Google and Tesla being mentioned in more negative posts\n",
    "- products: posts mentioning Dell laptops are mostly negative, Linux is the most positively discussed product of the top 10 most mentioned (average of 1.24)\n",
    "- persons: Elon Musk is by far the most discussed person in either a neutral or highly diverse fashion, Michael likely refers to Dell-CEO Michael Dell\n",
    "- location: India most frequently mentioned and associated with negative sentiment, Austin is the most positively discussed location in top 10\n",
    "- event: most mentioned event is the CDW executive summit, holidays are also in top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time-based plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting general time based overview\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plotting the overall sentiment using the weekly average\n",
    "weekly_sentiment = df_dell_p.groupby(pd.Grouper(key=\"Datetime\", freq=\"W\"))[\"bert_pred_sa\"].mean().reset_index()\n",
    "plt.plot(weekly_sentiment[\"Datetime\"], weekly_sentiment[\"bert_pred_sa\"], marker=\"o\", linestyle=\"dashed\", color=\"black\", label=\"overall\")\n",
    "\n",
    "# plotting the average sentiment for all rows mentioning the entities, adding colors dynamically\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(unique_entities))\n",
    "for i, entity_type in enumerate(unique_entities):\n",
    "    entity_df = df_dell_p[df_dell_p[\"entity_types\"].apply(lambda x: entity_type in x)]\n",
    "    weekly_sentiment_filtered = entity_df.groupby(pd.Grouper(key=\"Datetime\", freq=\"W\"))[\"bert_pred_sa\"].mean().reset_index()\n",
    "    plt.plot(weekly_sentiment_filtered[\"Datetime\"], weekly_sentiment_filtered[\"bert_pred_sa\"], \n",
    "             marker=\"o\", linestyle=\"-\", color=colors[i], label=entity_type)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Sentiment Over Time by Entity Type\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sentiment\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more specific time-based insights\n",
    "def plot_sentiment_for_entities_over_time(df, entities):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    if isinstance(entities, str):\n",
    "        entity_df = df[df[\"entity_types\"].apply(lambda x: entities in x)]\n",
    "\n",
    "        weekly_sentiment = entity_df.groupby(pd.Grouper(key=\"Datetime\", freq=\"W\"))[\"bert_pred_sa\"].mean().reset_index()\n",
    "        plt.plot(weekly_sentiment[\"Datetime\"], weekly_sentiment[\"bert_pred_sa\"], \n",
    "                 marker=\"o\", linestyle=\"-\", color=\"blue\", label=entities)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(f\"Sentiment Over Time for Posts mentioning a {entities.capitalize()}\")\n",
    "    \n",
    "    if isinstance(entities, list):\n",
    "        entity_df = df[df[\"entity_types\"].apply(lambda x: all(ent in x for ent in entities))]\n",
    "\n",
    "        weekly_sentiment = entity_df.groupby(pd.Grouper(key=\"Datetime\", freq=\"W\"))[\"bert_pred_sa\"].mean().reset_index()\n",
    "        plt.plot(weekly_sentiment[\"Datetime\"], weekly_sentiment[\"bert_pred_sa\"], \n",
    "                 marker=\"o\", linestyle=\"-\", color=\"blue\", label=\", \".join([ent for ent in entities]))\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(f\"Sentiment Over Time for Posts mentioning {\", \".join([ent.capitalize() for ent in entities])}\")\n",
    "        \n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Sentiment\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_for_entities_over_time(df_dell_p, \"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_for_entities_over_time(df_dell_p, [\"corporation\", \"product\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusions:\n",
    "- overall sentiment is relatively consistently hovering around 0.9 with the peak being reached in May 2022 (~1.1), lowest overall sentiment in January 2022 (~0.75)\n",
    "- posts mentioning events are generally the most positive except for a negative start at the beginning of 2022, but relatively small sample size (704 posts detected as mentioning an event)\n",
    "- persons are discussed with mostly positive sentiment over entire time period (majority of weekly averages > 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
