{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub transformers torch accelerate datasets peft nervaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama2_7b_t = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token definitions for both tokenizer and model allow for batch size > 1\n",
    "llama2_7b_t.pad_token = llama2_7b_t.eos_token\n",
    "#model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tner/tweetner7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetner7_train = ds[\"train_all\"].to_pandas()\n",
    "tweetner7_test = ds[\"test_2021\"].to_pandas()\n",
    "\n",
    "tweetner7_big = pd.concat([tweetner7_train, tweetner7_test])\n",
    "len(tweetner7_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetner7_validation20 = ds[\"validation_2020\"].to_pandas()\n",
    "tweetner7_validation21 = ds[\"validation_2021\"].to_pandas()\n",
    "tweetner7_test20 = ds[\"test_2020\"].to_pandas()\n",
    "\n",
    "tweetner7_bigger = pd.concat([tweetner7_big, tweetner7_validation20, tweetner7_validation21, tweetner7_test20])\n",
    "len(tweetner7_bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict_tweetner = {\n",
    "    0: \"B-corporation\",\n",
    "    1: \"B-creative_work\",\n",
    "    2: \"B-event\",\n",
    "    3: \"B-group\",\n",
    "    4: \"B-location\",\n",
    "    5: \"B-person\",\n",
    "    6: \"B-product\",\n",
    "    7: \"I-corporation\",\n",
    "    8: \"I-creative_work\",\n",
    "    9: \"I-event\",\n",
    "    10: \"I-group\",\n",
    "    11: \"I-location\",\n",
    "    12: \"I-person\",\n",
    "    13: \"I-product\",\n",
    "    14: \"O\"\n",
    "}\n",
    "\n",
    "def tags_to_labels(col):\n",
    "    result = []\n",
    "    for i in col:\n",
    "        label = entity_dict_tweetner[i]\n",
    "        result.append(label)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning with tweetner7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(tweetner7_bigger)\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(entity_dict_tweetner.values())\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = llama2_7b_t(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Special tokens\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:  # First token of a word\n",
    "                label_ids.append(label[word_id])\n",
    "            else:  # Subword tokens\n",
    "                label_ids.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of the low-rank adaptation\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # target modules for LoRA\n",
    "    lora_dropout=0.1,  # dropout rate\n",
    "    bias=\"none\",  # bias handling\n",
    "    task_type=\"TOKEN_CLS\" # NER is a form of token classification\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # flattening predictions and labels\n",
    "    true_labels = [id_to_label[label] for label in labels.flatten() if label != -100]\n",
    "    true_predictions = [id_to_label[pred] for (pred, label) in zip(predictions.flatten(), labels.flatten()) if label != -100]\n",
    "\n",
    "    # calculating metrics\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-llama-2-7b\",\n",
    "    evaluation_strategy=\"epoch\",   # evaluating at every epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=128, # adjust batch size depending on resources,\n",
    "    per_device_eval_batch_size=128,  # high batch size because NER needs less RAM than SA\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500, # higher number to save time\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=llama2_7b_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 2 epochs, tweetner7-big\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7\")\n",
    "llama2_7b_t.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 3 epochs, tweetner7-big\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7-3\")\n",
    "llama2_7b_t.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 4 epochs, tweetner7bigger\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7-4\")\n",
    "llama2_7b_t.save_pretrained(\"ner-llama-2-7b-finetuned-tweetner7-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "model_ft = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"ner-llama-2-7b-finetuned-tweetner7-4\"\n",
    ")\n",
    "\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"ner-llama-2-7b-finetuned-tweetner7-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_dataset)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_pred(model, tokenizer, df, text_col, tag_col=None, max_length=128):\n",
    "    # setting padding tokens\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    model.to(model.device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        if start_idx >= len(df):\n",
    "            break\n",
    "\n",
    "        batch_tokens = df[text_col][start_idx:end_idx].tolist()\n",
    "\n",
    "        # tokenizing with the same parameters as during training\n",
    "        batch_inputs = tokenizer(\n",
    "            batch_tokens,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=True,  # very important for token classification\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch_inputs = {key: value.to(model.device) for key, value in batch_inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_inputs)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            processed_predictions = []\n",
    "            for j, preds in enumerate(batch_predictions):\n",
    "                word_ids = tokenizer(batch_tokens[j], is_split_into_words=True).word_ids()\n",
    "                previous_word_id = None\n",
    "                word_preds = []\n",
    "\n",
    "                for word_id, pred in zip(word_ids, preds):\n",
    "                    if word_id is None:  # handling special tokens\n",
    "                        continue\n",
    "                    elif word_id != previous_word_id:  # first token of a word\n",
    "                        word_preds.append(pred.item())\n",
    "                    previous_word_id = word_id\n",
    "\n",
    "                processed_predictions.append(word_preds)\n",
    "\n",
    "            all_predictions.extend(processed_predictions)\n",
    "\n",
    "    df[\"llama_ft_prediction\"] = all_predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = llama_pred(model_ft, tokenizer_ft, test_df, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"true_labels\"] = test_df[\"tags\"].apply(tags_to_labels)\n",
    "test_df[\"pred_labels\"] = test_df[\"llama_ft_prediction\"].apply(tags_to_labels)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "true = test_df[\"true_labels\"].values.tolist()\n",
    "pred = test_df[\"pred_labels\"].values.tolist()\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=[\"corporation\", \"creative_work\", \"event\", \"group\", \"location\", \"person\", \"product\"], loader=\"list\")\n",
    "\n",
    "results, results_by_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# function returns dictionary containing precision, recall and f1-score\n",
    "def evaluate_ner_predictions(df, true_col=\"tags\", pred_col=\"llama_ft_prediction\"):\n",
    "    y_true_flat = []\n",
    "    y_pred_flat = []\n",
    "\n",
    "    for true_seq, pred_seq in zip(df[true_col], df[pred_col]):\n",
    "       # skipping padding/special tokens (marked as -100 in ground truth)\n",
    "       for true_label, pred_label in zip(true_seq, pred_seq):\n",
    "            if true_label != -100:\n",
    "               y_true_flat.append(true_label)\n",
    "               y_pred_flat.append(pred_label)\n",
    "\n",
    "    precision = precision_score(y_true_flat, y_pred_flat, average='weighted')\n",
    "    recall = recall_score(y_true_flat, y_pred_flat, average='weighted')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(classification_report(y_true_flat, y_pred_flat))\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_ner_predictions(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech = pd.read_csv(\"tweets_bigtech_10ksample.csv\")\n",
    "df_tweets_bigtech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech[\"tokens\"] = df_tweets_bigtech[\"text\"].apply(lambda x: x.split())\n",
    "df_tweets_bigtech = llama_pred(model_ft, tokenizer_ft, df_tweets_bigtech, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech[\"pred_labels\"] = df_tweets_bigtech[\"llama_ft_prediction\"].apply(tags_to_labels)\n",
    "df_tweets_bigtech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech.to_csv(\"10k_tweets_bigtech_after_llama_ner.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine tuning with corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_corpus = pd.read_json(\"NER_corpus.json\", orient=\"records\")\n",
    "df_corpus = df_corpus.rename(columns={\"labels\":\"original_labels\"})\n",
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    0: \"B-corporation\",\n",
    "    1: \"B-event\",\n",
    "    2: \"B-location\",\n",
    "    3: \"B-person\",\n",
    "    4: \"B-product\",\n",
    "    5: \"I-corporation\",\n",
    "    6: \"I-event\",\n",
    "    7: \"I-location\",\n",
    "    8: \"I-person\",\n",
    "    9: \"I-product\",\n",
    "    10: \"O\"\n",
    "}\n",
    "\n",
    "label_list = list(entity_dict.values())\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining length to determine maxlength\n",
    "df_corpus[\"tokens\"].str.len().agg(['mean','max','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_corpus[df_corpus[\"tokens\"].apply(len)>=128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama_t = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "llama_t.pad_token = llama_t.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "llama = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "llama.config.pad_token_id = llama.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "llama = prepare_model_for_kbit_training(llama)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank of the low-rank adaptation\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],  # target modules for LoRA\n",
    "    lora_dropout=0.05,  # dropout rate\n",
    "    bias=\"none\",  # bias handling\n",
    "    task_type=\"TOKEN_CLS\" # NER is a form of token classification\n",
    ")\n",
    "\n",
    "llama = get_peft_model(llama, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = llama_t(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset = Dataset.from_pandas(df_corpus)\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split1 = tokenized_datasets.train_test_split(test_size=0.15, seed=42)\n",
    "train_val_dataset = train_test_split1['train']\n",
    "test_dataset = train_test_split1['test']\n",
    "\n",
    "train_test_split2 = train_val_dataset.train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = train_test_split2['train']\n",
    "val_dataset = train_test_split2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_to_label)\n",
    "label_names = list(entity_dict.values())\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "     predictions, labels = p\n",
    "     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "     # flattening predictions and labels\n",
    "     true_labels = [id_to_label[label] for label in labels.flatten() if label != -100]\n",
    "     true_predictions = [id_to_label[pred] for (pred, label) in zip(predictions.flatten(), labels.flatten()) if label != -100]\n",
    "\n",
    "     # calculating metrics\n",
    "     report = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "     return {\n",
    "         \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "         \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "         \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "     }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-llama-2-7b\",\n",
    "    evaluation_strategy=\"epoch\", # evaluating at every epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32, # adjust batch size depending on resources\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llama,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=llama_t,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./llama2-7b-after-ner-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-llama-2-7b\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=7,\n",
    "    save_total_limit=7,\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llama,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=llama_t\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_llama = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "def calculate_metrics(df, true_label_col, pred_label_col):\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    for true, pred in zip(df[true_label_col], df[pred_label_col]):\n",
    "        all_true.extend(true)\n",
    "        all_pred.extend(pred[:len(true)])\n",
    "\n",
    "\n",
    "    report = classification_report(all_true, all_pred, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "val_df = (val_dataset).to_pandas()\n",
    "\n",
    "folders = [141, 282, 423, 564, 705, 846, 980]\n",
    "for epoch in range(7):\n",
    "    foldernr = folders[epoch]\n",
    "    checkpoint_dir = f\"./ner-llama-2-7b/checkpoint-{str(foldernr)}\"\n",
    "    print(f\"Loading model from {checkpoint_dir}\")\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    checkpoint_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "    val_df_copy = val_df.copy()\n",
    "    val_df_with_preds = llama_pred(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        val_df_copy,\n",
    "        text_col=\"tokens\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    metrics = calculate_metrics(\n",
    "        val_df_with_preds,\n",
    "        true_label_col=\"tags\",\n",
    "        pred_label_col=\"llama_ft_prediction\"\n",
    "    )\n",
    "\n",
    "    f1 = metrics[\"f1\"]\n",
    "    precision = metrics[\"precision\"]\n",
    "    recall = metrics[\"recall\"]\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(f\"Best epoch according to F1-score: {best_epoch+1}, F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_llama = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "llama_corpus = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./llama2-7b-after-ner-corpus\"\n",
    ")\n",
    "\n",
    "llama_corpus_t = AutoTokenizer.from_pretrained(\"./llama2-7b-after-ner-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = test_dataset.to_pandas()\n",
    "corpus_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted version to handle numpy arrays, probably due to json storing\n",
    "def llama_pred(model, tokenizer, df, text_col, max_length=128):\n",
    "    # setting padding tokens\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # moving model to GPU and setting it to evaluation-mode\n",
    "    model.to(model.device)\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        if start_idx >= len(df):\n",
    "            break\n",
    "\n",
    "        # converting numpy-arrays to lists\n",
    "        batch_tokens = [tokens.tolist() if hasattr(tokens, 'tolist') else tokens for tokens in df[text_col][start_idx:end_idx]]\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            batch_tokens,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        batch_inputs = {key: value.to(model.device) for key, value in encodings.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_inputs)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            processed_predictions = []\n",
    "            for j, (tokens, preds) in enumerate(zip(batch_tokens, batch_predictions)):\n",
    "                word_ids = encodings.word_ids(batch_index=j)\n",
    "                previous_word_id = None\n",
    "                word_preds = []\n",
    "\n",
    "                word_pred_map = {}\n",
    "                for word_id, pred in zip(word_ids, preds):\n",
    "                  pred_id = pred.item()\n",
    "                  if word_id is not None:  # Skip special tokens\n",
    "                      if word_id not in word_pred_map:\n",
    "                          word_pred_map[word_id] = []\n",
    "                      word_pred_map[word_id].append(pred.item())\n",
    "\n",
    "                for word_idx in sorted(word_pred_map.keys()):\n",
    "                    predictions = word_pred_map[word_idx]\n",
    "                    most_common = max(set(predictions), key=predictions.count)\n",
    "                    word_preds.append(most_common)\n",
    "\n",
    "                processed_predictions.append(word_preds)\n",
    "\n",
    "            all_predictions.extend(processed_predictions)\n",
    "\n",
    "    df[\"llama_ft_prediction\"] = all_predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = llama_pred(llama_corpus, llama_corpus_t, corpus_test, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "corpus_test[\"true_labels\"] = corpus_test[\"tags\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "corpus_test[\"pred_labels\"] = corpus_test[\"llama_ft_prediction\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "\n",
    "true = corpus_test[\"true_labels\"].values.tolist()\n",
    "pred = corpus_test[\"pred_labels\"].values.tolist()\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=[\"corporation\", \"event\", \"location\", \"person\", \"product\"], loader=\"list\")\n",
    "\n",
    "results, results_by_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in results_by_tag.keys():\n",
    "  print(f\"{ent}: {results_by_tag[ent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_ner_predictions(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after new method with checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking detailed entity-level performance of model after 3, 4 and 5 epochs on test set\n",
    "\n",
    "# first loading base model, then reloading fine-tuned model on top of it with PEFT\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_llama = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "llama_corpus_3 = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    \"./ner-llama-2-7b/checkpoint-423\"\n",
    ")\n",
    "\n",
    "llama_corpus_3_t = AutoTokenizer.from_pretrained(\"./ner-llama-2-7b/checkpoint-423\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test3 = llama_pred(llama_corpus_3, llama_corpus_3_t, corpus_test, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "corpus_test3[\"true_labels\"] = corpus_test3[\"tags\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "corpus_test3[\"pred_labels\"] = corpus_test3[\"llama_ft_prediction\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "\n",
    "true = corpus_test3[\"true_labels\"].values.tolist()\n",
    "pred = corpus_test3[\"pred_labels\"].values.tolist()\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=[\"corporation\", \"event\", \"location\", \"person\", \"product\"], loader=\"list\")\n",
    "\n",
    "results, results_by_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in results_by_tag.keys():\n",
    "  print(f\"{ent}: {results_by_tag[ent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_corpus_4 = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    \"./ner-llama-2-7b/checkpoint-564\"\n",
    ")\n",
    "\n",
    "llama_corpus_4_t = AutoTokenizer.from_pretrained(\"./ner-llama-2-7b/checkpoint-564\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test4 = llama_pred(llama_corpus_4, llama_corpus_4_t, corpus_test, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test4[\"true_labels\"] = corpus_test4[\"tags\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "corpus_test4[\"pred_labels\"] = corpus_test4[\"llama_ft_prediction\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "\n",
    "true = corpus_test4[\"true_labels\"].values.tolist()\n",
    "pred = corpus_test4[\"pred_labels\"].values.tolist()\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=[\"corporation\", \"event\", \"location\", \"person\", \"product\"], loader=\"list\")\n",
    "\n",
    "results, results_by_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in results_by_tag.keys():\n",
    "  print(f\"{ent}: {results_by_tag[ent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_ner_predictions(corpus_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_corpus_5 = PeftModel.from_pretrained(\n",
    "    base_llama,\n",
    "    \"./ner-llama-2-7b/checkpoint-705\"\n",
    ")\n",
    "\n",
    "llama_corpus_5_t = AutoTokenizer.from_pretrained(\"./ner-llama-2-7b/checkpoint-705\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test5 = llama_pred(llama_corpus_5, llama_corpus_5_t, corpus_test, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test5[\"true_labels\"] = corpus_test5[\"tags\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "corpus_test5[\"pred_labels\"] = corpus_test5[\"llama_ft_prediction\"].apply(lambda x: [id_to_label[i] for i in x])\n",
    "\n",
    "true = corpus_test5[\"true_labels\"].values.tolist()\n",
    "pred = corpus_test5[\"pred_labels\"].values.tolist()\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=[\"corporation\", \"event\", \"location\", \"person\", \"product\"], loader=\"list\")\n",
    "\n",
    "results, results_by_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in results_by_tag.keys():\n",
    "  print(f\"{ent}: {results_by_tag[ent]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
