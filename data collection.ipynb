{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Processing for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 million random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset of 1.6 million random tweets\n",
    "# https://www.kaggle.com/datasets/i191796majid/tweets\n",
    "df_rd_tweets = pd.read_csv(\"./analysis_data/1.6 million random tweets/train.csv/train.csv\")\n",
    "df_rd_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rd_tweets[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Tesla\", \"Apple\", \"iPhone\", \"MacBook\", \"AirPods\", \"Microsoft\", \"Windows\", \"Meta\", \"Twitter\", \"X\", \"Facebook\", \"Instagram\", \"Elon Musk\", \"Bill Gates\", \"Steve Jobs\", \"Tim Cook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_words = dict.fromkeys(words, 0)\n",
    "\n",
    "for key in d_words.keys():\n",
    "    d_words[key] = df_rd_tweets[\"tweet\"].str.contains(key).sum() \n",
    "\n",
    "print(d_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit r/Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset of comments from the subreddit r/Technology\n",
    "# https://www.kaggle.com/datasets/thedevastator/uncovering-technology-insights-through-reddit-di\n",
    "df_rtechnology = pd.read_csv(\"./analysis_data/reddit technology.csv\")\n",
    "df_rtechnology.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rtechnology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rtechnology[\"score\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rtechnology[df_rtechnology[\"score\"]<1000])/len(df_rtechnology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_words2 = dict.fromkeys(words, 0)\n",
    "\n",
    "for key in d_words2.keys():\n",
    "    d_words2[key] = df_rtechnology[\"title\"].str.contains(key).sum() \n",
    "\n",
    "print(d_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Tech Companies - Tweet Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Tech Companies - Tweet Sentiment \n",
    "# https://www.kaggle.com/datasets/wjia26/big-tech-companies-tweet-sentiment/data\n",
    "import pandas as pd\n",
    "\n",
    "df_tweets_bigtech = pd.read_csv(\"./analysis_data/Big Tech Companies - Tweet Sentiment/Bigtech - 12-07-2020 till 19-09-2020/Bigtech - 12-07-2020 till 19-09-2020.csv\")\n",
    "df_tweets_bigtech.dropna(inplace=True)\n",
    "df_tweets_bigtech.drop_duplicates(inplace=True)\n",
    "df_tweets_bigtech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets_bigtech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech[\"created_at\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_polarity = df_tweets_bigtech[\"polarity\"].min()\n",
    "max_polarity = df_tweets_bigtech[\"polarity\"].max()\n",
    "print(min_polarity, max_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting polarity to figure out tresholds for sentiment column\n",
    "df_tweets_bigtech[\"polarity\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 negative\n",
    "# 1 neutral\n",
    "# 2 positive\n",
    "# function converting polarity to sentiment\n",
    "def polarity_to_sentiment(polarity):\n",
    "    if -0.2 <= polarity <= 0.2:\n",
    "        return 1\n",
    "    if polarity > 0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_tweets_bigtech[\"labels\"] = df_tweets_bigtech[\"polarity\"].apply(polarity_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtech_negative = df_tweets_bigtech[df_tweets_bigtech[\"labels\"]==0].sample(4700, random_state=42)\n",
    "\n",
    "bigtech_neutral = df_tweets_bigtech[df_tweets_bigtech[\"labels\"]==1].sample(4700, random_state=42)\n",
    "\n",
    "bigtech_positive = df_tweets_bigtech[df_tweets_bigtech[\"labels\"]==2].sample(4700, random_state=42)\n",
    "\n",
    "# .sample to shuffle the rows\n",
    "df_tweets_bigtech_sample = pd.concat([bigtech_negative, bigtech_neutral, bigtech_positive], axis=0, ignore_index=True).sample(frac=1, random_state=42).reset_index()\n",
    "df_tweets_bigtech_sample = df_tweets_bigtech_sample[[\"text\", \"labels\"]]\n",
    "df_tweets_bigtech_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweets_bigtech_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_sample.to_json(\"tweets_bigtech_sample.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data for final application that does not overlap with training data\n",
    "\n",
    "all_texts = df_tweets_bigtech[\"text\"].unique()\n",
    "corpus_texts = set(df_tweets_bigtech_sample[\"text\"].unique())\n",
    "remaining_texts = [x for x in all_texts if x not in corpus_texts]\n",
    "\n",
    "remaining_df = df_tweets_bigtech[df_tweets_bigtech[\"text\"].isin(remaining_texts)]\n",
    "\n",
    "# .sample to shuffle the rows\n",
    "df_tweets_bigtech_app = remaining_df.sample(10000, random_state=42).reset_index()\n",
    "\n",
    "df_tweets_bigtech_app[\"tokens\"] = df_tweets_bigtech_app[\"text\"].apply(lambda text: text.split())\n",
    "df_tweets_bigtech_app = df_tweets_bigtech_app[[\"text\", \"tokens\", \"labels\"]]\n",
    "\n",
    "df_tweets_bigtech_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_app.to_json(\"tweets_bigtech_10k_application.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_words3 = dict.fromkeys(words, 0)\n",
    "\n",
    "for key in d_words3.keys():\n",
    "    d_words3[key] = df_tweets_bigtech[\"text\"].str.contains(key).sum() \n",
    "\n",
    "print(d_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand Sentiment Analysis Dataset \n",
    "# https://www.kaggle.com/datasets/tusharpaul2001/brand-sentiment-analysis-dataset?select=Dataset+-+Train.csv\n",
    "df_brd_sa = pd.read_csv(\"./analysis_data/Brand Sentiment Analysis Dataset/Dataset - Train.csv\")\n",
    "df_brd_sa = df_brd_sa[[\"tweet_text\", \"is_there_an_emotion_directed_at_a_brand_or_product\"]]\n",
    "df_brd_sa.dropna(inplace=True)\n",
    "df_brd_sa.drop_duplicates(inplace=True)\n",
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_brd_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa_test = pd.read_csv(\"./analysis_data/Brand Sentiment Analysis Dataset/Dataset - Test.csv\")\n",
    "len(df_brd_sa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-document has no labels\n",
    "df_brd_sa_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urlextract import URLExtract\n",
    "\n",
    "extractor = URLExtract()\n",
    "\n",
    "def format_tweet(tweet):\n",
    "    # mask web urls\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, \"{{URL}}\")\n",
    "    # format twitter account\n",
    "    tweet = re.sub(r\"\\b(\\s*)(@[\\S]+)\\b\", r'\\1{\\2@}', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brd_sa[\"tweet_text\"] = df_brd_sa.apply(lambda x: format_tweet(str(x[\"tweet_text\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting labels to numeric\n",
    "df_brd_sa[\"labels\"] = df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace({\"Negative emotion\" : 0, \"Positive emotion\" : 2, \"No emotion toward brand or product\" : 1, \"I can't tell\" : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.rename(columns={\"tweet_text\" : \"text\"}, inplace=True)\n",
    "df_brd_sa = df_brd_sa[[\"text\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sample with equal sentiment distribution\n",
    "brd_sa_negative = df_brd_sa[df_brd_sa[\"labels\"]==0].sample(500, random_state=42)\n",
    "\n",
    "brd_sa_neutral = df_brd_sa[df_brd_sa[\"labels\"]==1].sample(500, random_state=42)\n",
    "\n",
    "brd_sa_positive = df_brd_sa[df_brd_sa[\"labels\"]==2].sample(500, random_state=42)\n",
    "\n",
    "# .sample to shuffle the rows\n",
    "df_brd_sa_sample = pd.concat([brd_sa_negative, brd_sa_neutral, brd_sa_positive], axis=0, ignore_index=True).sample(frac=1, random_state=42).reset_index()\n",
    "df_brd_sa_sample = df_brd_sa_sample[[\"text\", \"labels\"]]\n",
    "df_brd_sa_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_brd_sa_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brd_sa.to_csv(\"./SA_data/brand_sentiment_analysis_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_words4 = dict.fromkeys(words, 0)\n",
    "\n",
    "for key in d_words4.keys():\n",
    "    d_words4[key] = df_brd_sa[\"text\"].str.contains(key).sum() \n",
    "\n",
    "print(d_words4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets Sentiment Classification \n",
    "# https://www.kaggle.com/datasets/bhrt97/tweets-sentiment-classification?resource=download\n",
    "df_tweets_sc = pd.read_csv(\"./SA_data/Tweets Sentiment Classification/train.csv\", index_col=\"id\")\n",
    "\n",
    "# test-dataset has no labels \n",
    "#df_tweets_sc_ts = pd.read_csv(\"./SA_data/Tweets Sentiment Classification/test.csv\", index_col=\"id\")\n",
    "#df_tweets_sc = pd.concat([df_tweets_sc_tr, df_tweets_sc_ts])\n",
    "df_tweets_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_sc.dropna(inplace=True)\n",
    "len(df_tweets_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_sc[\"label\"] = df_tweets_sc[\"label\"].astype(int)\n",
    "\n",
    "# replacing 1 with 2 because in other data 2 = positive sentiment\n",
    "df_tweets_sc[\"label\"] = df_tweets_sc[\"label\"].replace({1:2})\n",
    "df_tweets_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_sc[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_sc[\"tweet\"] = df_tweets_sc.apply(lambda x: format_tweet(str(x[\"tweet\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweets_sc.to_csv(\"./SA_data/Tweets Sentiment Classification/tweets_sentiment_classification_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter US Airline Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data of tweets directed at US airlines\n",
    "# https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "import pandas as pd\n",
    "\n",
    "df_airline = pd.read_csv(\"./SA_data/Twitter US Airline Sentiment.csv\")\n",
    "df_airline = df_airline[[\"text\", \"airline_sentiment\"]]\n",
    "df_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_airline))\n",
    "df_airline.dropna(inplace=True)\n",
    "df_airline.drop_duplicates(inplace=True)\n",
    "print(len(df_airline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline[\"airline_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentiment column to numeric structure\n",
    "df_airline[\"airline_sentiment\"].replace({\"negative\" : 0, \"neutral\" : 1, \"positive\" : 2}, inplace=True)\n",
    "df_airline.rename(columns={\"airline_sentiment\" : \"labels\"}, inplace=True)\n",
    "df_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sample with equal sentiment distribution\n",
    "df_airline_negative = df_airline[df_airline[\"labels\"]==0].sample(1667, random_state=42)\n",
    "df_airline_neutral = df_airline[df_airline[\"labels\"]==1].sample(1668, random_state=42)\n",
    "df_airline_positive = df_airline[df_airline[\"labels\"]==2].sample(1667, random_state=42)\n",
    "\n",
    "# .sample to shuffle the rows\n",
    "df_airline_sample = pd.concat([df_airline_negative, df_airline_neutral, df_airline_positive], axis=0, ignore_index=True).sample(frac=1, random_state=42).reset_index()\n",
    "df_airline_sample = df_airline_sample[[\"text\", \"labels\"]]\n",
    "df_airline_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_airline_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment and Emotions labeled tweets - Dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment and Emotions labelled tweets - Dell \n",
    "# https://www.kaggle.com/datasets/ankitkumar2635/sentiment-and-emotions-of-tweets\n",
    "import pandas as pd\n",
    "\n",
    "df_dell = pd.read_csv(\"./SA_data/sentiment-emotion-labelled_Dell_tweets.csv\")\n",
    "df_dell.dropna(inplace=True)\n",
    "df_dell.drop_duplicates(inplace=True)\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell = df_dell[[\"Datetime\", \"Text\", \"sentiment\"]]\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell[\"Datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# counting most frequent mentions\n",
    "df_dell[\"mentions\"] = df_dell[\"Text\"].apply(lambda x: re.findall(r'@\\S+', str(x)) if pd.notna(x) else [])\n",
    "all_mentions = [mention for mentions in df_dell[\"mentions\"] for mention in mentions]\n",
    "mention_counts = pd.Series(Counter(all_mentions))\n",
    "mention_counts = mention_counts.sort_values(ascending=False)\n",
    "print(mention_counts[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing urls and user mentions while keeping company mentions\n",
    "\n",
    "mentions_to_keep = [\"Dell\", \"Delltech\", \"Dellcares\", \"Dell,\", \"HP\", \"Microsoft\", \"Apple\", \"Logitech\", \"Google\", \"Lenovo\", \"Tesla\", \"Intel\", \"Alienware\", \"Emc\"]\n",
    "\n",
    "def format_tweet_new_keep(tweet):\n",
    "    if not isinstance(tweet, str):  \n",
    "        return \"\"\n",
    "\n",
    "    tokens = tweet.split()\n",
    "    tokens = [token[1:].capitalize() if token.startswith(\"@\") and token[1:].capitalize() in mentions_to_keep else token for token in tokens]\n",
    "\n",
    "    tweet = \" \".join(tokens)\n",
    "\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, \"{{URL}}\")\n",
    "\n",
    "    tweet = re.sub(r'@\\S+', \"{{MENTION}}\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "df_dell[\"text\"] = df_dell.apply(lambda x: format_tweet_new_keep(str(x[\"Text\"])), axis=1)\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell[\"tokens\"] = df_dell[\"text\"].apply(lambda x: x.split())\n",
    "df_dell = df_dell[[\"Datetime\", \"text\", \"tokens\", \"sentiment\"]]\n",
    "df_dell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dell.to_json(\"./SA_data/sentiment_dell_processed.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a corpus from multiple Sentiment-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets Big Tech, Brand Sentiment Analysis data and US Airline Sentiment all combined\n",
    "df_sa_corpus = pd.concat([df_tweets_bigtech_sample, df_brd_sa_sample, df_airline_sample], ignore_index=True).sample(frac=1, random_state=42).reset_index()\n",
    "df_sa_corpus = df_sa_corpus[[\"text\", \"labels\"]]\n",
    "df_sa_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_sa_corpus))\n",
    "df_sa_corpus.dropna(inplace=True)\n",
    "df_sa_corpus.drop_duplicates(inplace=True)\n",
    "print(len(df_sa_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa_corpus[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing urls and user mentions \n",
    "def format_tweet_new(tweet):\n",
    "    # mask web urls\n",
    "    urls = extractor.find_urls(tweet)\n",
    "    for url in urls:\n",
    "        tweet = tweet.replace(url, \"{{URL}}\")\n",
    "    # format twitter account\n",
    "    # recognizes tokens including @ at any place\n",
    "    tweet = re.sub(r'\\S*@\\S*', \"{{MENTION}}\", tweet)    \n",
    "    return tweet\n",
    "\n",
    "df_sa_corpus[\"text\"] = df_sa_corpus.apply(lambda x: format_tweet_new(str(x[\"text\"])), axis=1)\n",
    "df_sa_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sa_corpus.to_json(\"./SA_data/sentiment_corpus.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Processing for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WNUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset WNUT 2016\n",
    "# data from https://autonlp.ai/datasets/wnut-2016\n",
    "import pandas as pd\n",
    "wnut_2016_train = pd.read_csv(\"./NER_data/WNUT 2016 train.txt\", sep=\"\\t\", header=None, names=[\"words\", \"labels\"], skip_blank_lines=False)\n",
    "wnut_2016_test = pd.read_csv(\"./NER_data/WNUT 2016 test.txt\", sep=\"\\t\", header=None, names=[\"words\", \"labels\"], skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_2016 = pd.concat([wnut_2016_train, wnut_2016_test], axis=0)\n",
    "wnut_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_2016[\"sentence_id\"] = (pd.isna(wnut_2016.words)).cumsum()\n",
    "wnut_2016.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_2016.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked version to fix list mismatches between columns\n",
    "grouped_data = wnut_2016.groupby(\"sentence_id\").agg({\n",
    "    \"words\": list,  \"labels\": list}).reset_index()\n",
    "\n",
    "grouped_data.rename(columns={\"words\": \"tokens\", \"labels\": \"label_list\"}, inplace=True)\n",
    "\n",
    "wnut_2016_final = grouped_data[[\"tokens\", \"label_list\"]]\n",
    "\n",
    "wnut_2016_final[\"tokens_len\"] = wnut_2016_final[\"tokens\"].apply(len)\n",
    "wnut_2016_final[\"labels_len\"] = wnut_2016_final[\"label_list\"].apply(len)\n",
    "mismatched = wnut_2016_final[wnut_2016_final[\"tokens_len\"] != wnut_2016_final[\"labels_len\"]]\n",
    "\n",
    "print(f\"Rows with mismatched lengths: {len(mismatched)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wnut_2016_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading WNUT 2017 dataset\n",
    "# https://github.com/juand-r/entity-recognition-datasets/blob/master/data/WNUT17/CONLL-format/data/train/wnut17train.conll\n",
    "\n",
    "# function for creating a clean df from .conll-file\n",
    "def read_conll(filename):\n",
    "    df = pd.read_csv(filename,\n",
    "                    sep = \"\\t\", header = None, keep_default_na = False,\n",
    "                    names = [\"words\", \"labels\", \"chunk\", \"ne\"],\n",
    "                    quoting = 3, skip_blank_lines = False)\n",
    "    df[\"sentence_id\"] = (df.words == '').cumsum()\n",
    "    return df[df.words != '']\n",
    "\n",
    "wnut_2017_train = read_conll(\"./NER_data/wnut17train.conll\")\n",
    "wnut_2017_test = read_conll(\"./NER_data/wnut17test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_2017 = pd.concat([wnut_2017_train, wnut_2017_test], axis=0)\n",
    "wnut_2017 = wnut_2017[[\"words\", \"labels\", \"sentence_id\"]]\n",
    "wnut_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked version to fix list mismatches between columns\n",
    "grouped_data = wnut_2017.groupby(\"sentence_id\").agg({\n",
    "    \"words\": list,  \n",
    "    \"labels\": list \n",
    "}).reset_index()\n",
    "\n",
    "grouped_data.rename(columns={\"words\": \"tokens\", \"labels\": \"label_list\"}, inplace=True)\n",
    "\n",
    "wnut_2017_final = grouped_data[[\"tokens\", \"label_list\"]]\n",
    "\n",
    "# checking if there are mismatches in list length\n",
    "wnut_2017_final[\"tokens_len\"] = wnut_2017_final[\"tokens\"].apply(len)\n",
    "wnut_2017_final[\"labels_len\"] = wnut_2017_final[\"label_list\"].apply(len)\n",
    "mismatched = wnut_2017_final[wnut_2017_final[\"tokens_len\"] != wnut_2017_final[\"labels_len\"]]\n",
    "print(f\"Rows with mismatched lengths: {len(mismatched)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wnut_2017_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_2017_final[\"label_list\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_complete = pd.concat([wnut_2016_final, wnut_2017_final])\n",
    "len(wnut_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of tokens labeled as \"Outside\"\n",
    "all_wnut_labels = wnut_complete[\"label_list\"].explode().tolist()\n",
    "outside_wnut = all_wnut_labels.count(\"O\")\n",
    "print(f\"Percentange of labels that are O: {outside_wnut/len(all_wnut_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_complete[\"label_list\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing \"company\"-tag to \"corporation\" to match TweetNER\n",
    "wnut_complete[\"label_list\"] = wnut_complete[\"label_list\"].apply(\n",
    "    lambda x: [item.replace(\"B-company\", \"B-corporation\") if isinstance(item, str) else item for item in x]\n",
    ")\n",
    "wnut_complete[\"label_list\"] = wnut_complete[\"label_list\"].apply(\n",
    "    lambda x: [item.replace(\"I-company\", \"I-corporation\") if isinstance(item, str) else item for item in x]\n",
    ")\n",
    "\n",
    "#changing \"geo-loc\"-tag to \"location to match\"\n",
    "wnut_complete[\"label_list\"] = wnut_complete[\"label_list\"].apply(\n",
    "    lambda x: [item.replace(\"B-geo-loc\", \"B-location\") if isinstance(item, str) else item for item in x]\n",
    ")\n",
    "wnut_complete[\"label_list\"] = wnut_complete[\"label_list\"].apply(\n",
    "    lambda x: [item.replace(\"I-geo-loc\", \"I-location\") if isinstance(item, str) else item for item in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_complete[\"label_list\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wnut_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unwanted_entities(label_list):\n",
    "    unwanted_entities = [\"B-musicartist\", \"I-musicartist\", \"B-movie\", \"I-movie\", \"B-sportsteam\", \"I-sportsteam\", \n",
    "                     \"B-facility\", \"I-facility\", \"B-group\", \"I-group\", \"B-creative-work\", \"I-creative-work\", \n",
    "                     \"B-creative_work\", \"I-creative_work\", \"B-tvshow\", \"I-tvshow\", \"B-other\", \"I-other\"]\n",
    "    return [\"O\" if label in unwanted_entities else label for label in label_list]\n",
    "\n",
    "wnut_complete[\"label_list\"] = wnut_complete[\"label_list\"].apply(convert_unwanted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity-dict with only the wanted entities \n",
    "\n",
    "entity_dict = {\n",
    "    0: \"B-corporation\",\n",
    "    1: \"B-event\",\n",
    "    2: \"B-location\",\n",
    "    3: \"B-person\",\n",
    "    4: \"B-product\",\n",
    "    5: \"I-corporation\",\n",
    "    6: \"I-event\",\n",
    "    7: \"I-location\",\n",
    "    8: \"I-person\",\n",
    "    9: \"I-product\",\n",
    "    10: \"O\"\n",
    "}\n",
    "\n",
    "label_list = list(entity_dict.values())\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding tag-column from label-column with self-defined tags\n",
    "wnut_complete[\"tags\"] = wnut_complete[\"label_list\"].apply(lambda x: [label_to_id[i] for i in x])\n",
    "wnut_complete.rename(columns={\"label_list\" : \"labels\"}, inplace=True)\n",
    "wnut_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving processed data as csv\n",
    "wnut_complete.to_csv(\"./NER_data/wnut_complete_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TweetNER7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tweetner17 dataset from huggingface \n",
    "# https://huggingface.co/datasets/tner/tweetner7\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tner/tweetner7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetner7_train = ds[\"train_all\"].to_pandas()\n",
    "tweetner7_test21 = ds[\"test_2021\"].to_pandas()\n",
    "tweetner7_test20 = ds[\"test_2020\"].to_pandas()\n",
    "tweetner7_val20 = ds[\"validation_2020\"].to_pandas()\n",
    "tweetner7_val21 = ds[\"validation_2021\"].to_pandas()\n",
    "tweetner7_train.dropna(inplace=True)\n",
    "tweetner7_test21.dropna(inplace=True)\n",
    "tweetner7_test20.dropna(inplace=True)\n",
    "tweetner7_val20.dropna(inplace=True)\n",
    "tweetner7_val21.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetner7_all = pd.concat([tweetner7_train, tweetner7_test21, tweetner7_test20, tweetner7_val20, tweetner7_val21])\n",
    "tweetner7_all = tweetner7_all[[\"tokens\", \"tags\"]]\n",
    "tweetner7_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding label-column from tags\n",
    "entity_dict_tweetner = {\n",
    "    0: \"B-corporation\",\n",
    "    1: \"B-creative_work\",\n",
    "    2: \"B-event\",\n",
    "    3: \"B-group\",\n",
    "    4: \"B-location\",\n",
    "    5: \"B-person\",\n",
    "    6: \"B-product\",\n",
    "    7: \"I-corporation\",\n",
    "    8: \"I-creative_work\",\n",
    "    9: \"I-event\",\n",
    "    10: \"I-group\",\n",
    "    11: \"I-location\",\n",
    "    12: \"I-person\",\n",
    "    13: \"I-product\",\n",
    "    14: \"O\"\n",
    "}\n",
    "\n",
    "label_list_tweetner = list(entity_dict_tweetner.values())\n",
    "label_to_id_tweetner = {label: i for i, label in enumerate(label_list_tweetner)}\n",
    "id_to_label_tweetner = {i: label for label, i in label_to_id_tweetner.items()}\n",
    "\n",
    "tweetner7_all[\"labels\"] = tweetner7_all[\"tags\"].apply(lambda x: [id_to_label_tweetner[i] for i in x])\n",
    "tweetner7_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweetner7_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [\n",
    "        \"John Smith\", \"Emily Chen\", \"Michael Johnson\", \"Sarah Williams\",\n",
    "        \"David Lee\", \"Maria Rodriguez\", \"James Brown\", \"Davis\",\n",
    "        \"Robert Kim\", \"Jennifer Lopez\", \"Thomas Wilson\", \"Jessica Taylor\", \"Cook\",\n",
    "        \"Carlos Vega\", \"Aisha Patel\", \"Daniel Park\", \"Olivia Nguyen\", \"Musk\", \"Smith\"\n",
    "    ]\n",
    "\n",
    "corporations = [\n",
    "        \"Google\", \"Microsoft\", \"Apple\", \"Amazon\", \"Meta\", \"BlackBerry\",\n",
    "        \"IBM\", \"Tesla\", \"Netflix\", \"Walmart\", \"JP Morgan\",\n",
    "        \"Acme Corp\", \"TechSolutions\", \"Global Systems\", \"DataWorks\",\n",
    "        \"Quantum Industries\", \"NexGen\", \"FutureSpace\", \"EcoSystems\", \"Nokia\", \"Motorola\"\n",
    "    ]\n",
    "\n",
    "products = [\n",
    "        \"iPhone 13\", \"Galaxy S22\", \"Surface Pro\", \"PlayStation 5\", \"Xbox Series X\",\n",
    "        \"MacBook Air\", \"Echo Dot\", \"AirPods Pro\", \"Tesla Model 3\", \"iPad Mini\",\n",
    "        \"Dyson V11\", \"Fitbit Charge\", \"Nintendo Switch\", \"Kindle Paperwhite\",\n",
    "        \"Roomba i7\", \"GoPro Hero\", \"Bose QuietComfort\", \"Instant Pot\", \"Echo\", \"AirTag\", \"ThinkPad\"\n",
    "    ]\n",
    "\n",
    "events = [\n",
    "        \"CES 2023\", \"Web Summit\", \"SXSW\", \"TechCrunch Disrupt\", \"E3 Expo\",\n",
    "        \"Google I/O\", \"WWDC\", \"Consumer Electronics Show\", \"Mobile World Congress\",\n",
    "        \"Black Hat Conference\", \"DEF CON\", \"AWS re:Invent\", \"Game Developers Conference\",\n",
    "        \"Dreamforce\", \"Comic-Con\", \"Coachella\", \"New York Fashion Week\", \"GamesCom\", \"AI-Con\"\n",
    "    ]\n",
    "\n",
    "locations = [\n",
    "        \"New York\", \"San Francisco\", \"London\", \"Tokyo\", \"Berlin\",\n",
    "        \"Paris\", \"Sydney\", \"Toronto\", \"Chicago\", \"Seattle\",\n",
    "        \"Los Angeles\", \"Miami\", \"Singapore\", \"Hong Kong\", \"Milwaukee\",\n",
    "        \"Dubai\", \"Barcelona\", \"Austin\", \"Stockholm\", \"Seoul\", \"Vienna\"\n",
    "    ]\n",
    "\n",
    "templates = [\n",
    "        \"{person} from {corporation} announced that {product} will be showcased at {event}.\",\n",
    "        \"At {event}, {person} demonstrated how {product} is revolutionizing {corporation}'s approach in {location}.\",\n",
    "        \"{corporation} has selected {location} as the venue for {event}, where {person} will launch {product}.\",\n",
    "        \"The new {product} developed by {corporation} will be presented by {person} during {event} in {location}.\",\n",
    "        \"{person} confirmed that {corporation} will be expanding its {product} line.\",\n",
    "        \"According to {person}, {corporation}'s latest {product} has been well-received at {event} in {location}.\",\n",
    "        \"Reviews from {event} suggest that {person} made a strong case for {corporation}'s new {product} in the {location} market.\",\n",
    "        \"{corporation} is planning to open a {product} store in {location}, announced {person} at {event}.\",\n",
    "        \"The collaboration between {corporation} and {person} resulted in {product}, which will finally debut at {event} in {location}.\",\n",
    "        \"Attendees at {event} in {location} were very impressed when {person} revealed {corporation}'s innovative {product}! The clapping didnt stop\",\n",
    "        \"{person} traveled to {location} to promote {product} at {event} on behalf of {corporation}.\",\n",
    "        \"The {product} team from {corporation}, led by {person}, won first prize at {event} in {location}. Let's go!\",\n",
    "        \"Consumers in {location} can now purchase {product} after {corporation}'s expansion announcement by {person} at {event}.\",\n",
    "        \"{product} is the must-have gadget of the year.\",\n",
    "        \"I hate the new {product}, the older ones are much better.\",\n",
    "        \"Less than 2 hours until they announce the details on the {product} giveaway!\",\n",
    "        \"All eyes are on {corporation} after the announcement of their new {product}.\",\n",
    "        \"It's time for {person} to leave {corporation}. What is he even doing.\",\n",
    "        \"{corporation} has been selected as the top AI startup in {location}, wow!\",\n",
    "        \"I am having so many issues with the {product}. {corporation} needs to fix this!\",\n",
    "        \"Can not wait for {product} also. They should sell them down at {event}.\",\n",
    "        \"Whats happening at {corporation}? {person} really needs to step up.\",\n",
    "        \"{corporation} is giving free {product} to open source coders who are attending this meet-up.\",\n",
    "        \"{person} was right! The {product} from {corporation} is revolutionary!\",\n",
    "        \"Less than 2 hours until we announce the details on the {product} giveaway!\",\n",
    "        \"{corporation} CEO {person}: Newest {product} rollout will begin next month!\",\n",
    "        \"{corporation} has a temporary Retail Store in {location} for the {product} release today. Opens at 5pm.\",\n",
    "        \"{person} said that {corporation} is working on something big.\",\n",
    "        \"It's time for {person} to leave {corporation}. What is he even doing?\",\n",
    "        \"{corporation} just keeps raising the bar with every {product} they launch. Crazy!\",\n",
    "        \"{person} just hinted at new features in {corporation}'s upcoming {product}. I am hyped!\",\n",
    "        \"Rumors say {corporation} is releasing {product} soon.\",\n",
    "        \"Just watched the {corporation} keynote. {product} looks impressive.\",\n",
    "        \"Is it just me, or does {corporation}'s {product} feel rushed and unfinished?\",\n",
    "        \"The {product} is making me rethink my loyalty to {corporation}. Its not good.\",\n",
    "        \"Who else is gonna get the new {product} next month?\",\n",
    "        \"{corporation}'s industry party tonight was great for the launch of {product}.\",\n",
    "        \"Attending {event} this week! Can't wait to see what {corporation} unveils about their upcoming {product}. Anyone else going?\",\n",
    "        \"Is anyone else experiencing issues with the new {product} update? {corporation}'s support hasn't been helpful. #TechSupport\",\n",
    "        \"Just switched from {product} to {product} and the difference is incredible. {corporation} really cooked with this one!\",\n",
    "        \"Hot take: {corporation}'s approach to development is outdated. They need to focus more on usability if they want to compete with {corporation}\",\n",
    "        \"The new update to {product} completely revolutionized my workflow. Thanks {corporation} for fixing the issue! #ProductivityTech\",\n",
    "        \"Arrived at {event} in {location}! The {corporation} booth is already packed with people trying the new {product}. #TechConference\",\n",
    "        \"Just spotted {person} from {corporation} at a restaurant in {location} right after {event}. Tried to ask about {product} rumors but no comment!\",\n",
    "        \"I snuck into the VIP section at {event} in {location} and got a selfie with {person}! Check my Insta! #Winning\",\n",
    "        \"PSA: Free {product} giveaways at {corporation}'s booth at {event} in {location}! Run don't walk, peeps! I got the last blue one\",\n",
    "        \"This {product} launch line at {corporation}'s store in {location} is ridiculous. Been here 3hrs and moved like 10 feet, But I NEED it today! #TechAddict\",\n",
    "        \"Shoutout to the nice {corporation} rep at {event} in {location} who gave me an extra {product} for my kid! Some tech people are actually decent humans\",\n",
    "        \"My {product} just updated itself and now I can't find ANYTHING. Hey {corporation}, stop 'fixing' stuff that ain't broken! {person} needs to chill with these changes\",\n",
    "        \"Omg {person} just liked my tweet criticizing {corporation}'s {product}! Screenshot this before they realize and unlike!\",\n",
    "        \"The way {person} casually uses {product} in interviews makes it seem so cool, but when I bought it from {corporation} it's just... meh. Marketing wins again\",\n",
    "        \"new CEO {person} has really not done much yet at {corporation}, hasnt he?\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# reworked new function\n",
    "def generate_ner_dataset(num_examples, output_file=\"new_synthetic_ner_dataset.csv\", templates=templates,\n",
    "                         persons=persons, corporations=corporations, products=products, events=events, \n",
    "                         locations=locations):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        person = random.choice(persons)\n",
    "        corporation = random.choice(corporations)\n",
    "        product = random.choice(products)\n",
    "        event = random.choice(events)\n",
    "        location = random.choice(locations)\n",
    "        \n",
    "        sentence = template.format(\n",
    "            person=person,\n",
    "            corporation=corporation,\n",
    "            product=product,\n",
    "            event=event,\n",
    "            location=location\n",
    "        )\n",
    "        \n",
    "        tokens = []\n",
    "        labels = []\n",
    "        \n",
    "        raw_words = []\n",
    "        current_word = \"\"\n",
    "        for char in sentence:\n",
    "            if char.isalnum() or char in \"-'\":\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    raw_words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                if not char.isspace():\n",
    "                    raw_words.append(char)\n",
    "        if current_word:\n",
    "            raw_words.append(current_word)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(raw_words):\n",
    "            token = raw_words[i]\n",
    "            \n",
    "            found_entity = False\n",
    "            \n",
    "            if i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in persons:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-person\")\n",
    "                labels.append(\"I-person\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in products:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-product\")\n",
    "                labels.append(\"I-product\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 2 and f\"{raw_words[i]} {raw_words[i+1]} {raw_words[i+2]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                tokens.append(raw_words[i+2])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 3\n",
    "                found_entity = True\n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in events:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-event\")\n",
    "                labels.append(\"I-event\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            elif i < len(raw_words) - 1 and f\"{raw_words[i]} {raw_words[i+1]}\" in locations:\n",
    "                tokens.append(raw_words[i])\n",
    "                tokens.append(raw_words[i+1])\n",
    "                labels.append(\"B-location\")\n",
    "                labels.append(\"I-location\")\n",
    "                i += 2\n",
    "                found_entity = True\n",
    "                \n",
    "            if not found_entity:\n",
    "                if token in [name.split()[0] for name in persons]:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-person\")\n",
    "                    i += 1\n",
    "                elif token in corporations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-corporation\")\n",
    "                    i += 1\n",
    "                elif token in products:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-product\")\n",
    "                    i += 1\n",
    "                elif token in events:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-event\")\n",
    "                    i += 1\n",
    "                elif token in locations:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"B-location\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                    labels.append(\"O\")\n",
    "                    i += 1\n",
    "        \n",
    "        data.append({\"tokens\": tokens, \"labels\": labels, \"sentence\": sentence})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = generate_ner_dataset(5000)\n",
    "df_syn = df_syn[[\"tokens\", \"labels\"]]\n",
    "df_syn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding tag-column from label-column\n",
    "def convert_to_numeric(label_list):\n",
    "    return [label_to_id.get(label, -100) for label in label_list]\n",
    "\n",
    "df_syn[\"tags\"] = df_syn[\"labels\"].apply(convert_to_numeric)\n",
    "df_syn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all NER-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_corpus = pd.concat([wnut_complete, tweetner7_all, df_syn], axis=0).sample(frac=1, random_state=42).reset_index()\n",
    "df_ner_corpus = df_ner_corpus[[\"tokens\", \"labels\", \"tags\"]]\n",
    "df_ner_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_ner_corpus))\n",
    "df_ner_corpus.dropna(inplace=True)\n",
    "print(len(df_ner_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_corpus[\"labels\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_corpus[\"labels\"] = df_ner_corpus[\"labels\"].apply(convert_unwanted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_corpus[\"labels\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "557394/len(df_ner_corpus[\"labels\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwriting the tags-column from tweetner7-tags to self-defined tags to fit new label-column\n",
    "df_ner_corpus[\"tags\"] = df_ner_corpus[\"labels\"].apply(lambda x: [label_to_id[i] for i in x])\n",
    "df_ner_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing urls and user mentions in tokens-column\n",
    "def format_token_list(token_list):\n",
    "    formatted_tokens = []\n",
    "    for token in token_list:\n",
    "        # mask web urls\n",
    "        if extractor.find_urls(token):\n",
    "            formatted_tokens.append(\"{{URL}}\")\n",
    "        # format twitter mentions\n",
    "        elif re.search(r'\\S*@\\S*', token):\n",
    "            formatted_tokens.append(\"{{MENTION}}\")\n",
    "        else:\n",
    "            formatted_tokens.append(token)\n",
    "            \n",
    "    return formatted_tokens\n",
    "\n",
    "df_ner_corpus[\"tokens\"] = df_ner_corpus[\"tokens\"].apply(format_token_list)\n",
    "df_ner_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to json to preserve python data structures\n",
    "df_ner_corpus.to_json(\"./NER_data/NER_corpus.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
